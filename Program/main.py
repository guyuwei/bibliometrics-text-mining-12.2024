import matplotlib.pyplot as plt
import pandas as pd
import streamlit as st
import re
from datetime import datetime
from collections import defaultdict
import itertools
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from PIL.ImageOps import expand
from pandas.io.common import file_exists
import networkx as nx
import numpy as np
from wordcloud import WordCloud
import io
import base64
from scipy import stats
import seaborn as sns
try:
    from streamlit_option_menu import option_menu
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆ
    def option_menu(menu_title, options, icons=None, menu_icon=None, default_index=0, orientation="vertical", styles=None):
        return st.selectbox(menu_title, options, index=default_index)
import sys
import os
from Calculate_Anaysis.Calculate_Network import calculate_coupling
try:
    from st_on_hover_tabs import on_hover_tabs
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆ
    def on_hover_tabs(tabName, iconName, default_choice=0):
        return st.selectbox("é€‰æ‹©é¡µé¢", tabName, index=default_choice)
from Calculate_Anaysis.Calculate_Author import calculate_core_author_publication,calculate_number_of_authors_publication,process_author_data
from Calculate_Anaysis.Calculate_Country import calculate_collaboration_countries
from Calculate_Anaysis.Calculate_Publication import calculate_publications_per_year
from Calculate_Anaysis.Calculate_Sources import calculate_number_of_sources
from Calculate_Anaysis.Calculate_Keywords import calculate_number_of_keywords
from Calculate_Anaysis.Calculate_Reference import calculate_number_of_total_references_cited,filter_references_by_authors,extract_each_article_author_refauthor
from Calculate_Anaysis.Calculate_Citiation import calculate_number_of_total_Timescitedcount
from Calculate_Anaysis.Calculate_Year import exact_targetarticles_within_yearspan,calculate_age
from Calculate_Anaysis.Calculate_Advanced import AdvancedAnalysis, calculate_advanced_metrics
from Result_Visualization.Descriptive_Statistics import Form_Information_Description,shift_edited_df_into_list
from Result_Visualization.Publications_and_Authors import draw_author_density_visualiaztion,draw_author_overlay_visualiaztion,draw_author_network_visualiaztion
from Result_Visualization.Enhanced_Visualization import EnhancedVisualization, create_dashboard_summary
from Result_Visualization.Plot_Config import get_plot_config
from Documents_Processing.Uploading_Files import Load_TXT,Load_CSV
from Documents_Processing.Export_Functions import DataExporter, export_analysis_data, create_download_link, create_interactive_dashboard_export
from Documents_Processing.Web_Format import st_header, st_subheader, st_selectbox, st_card, st_tags, st_radio, st_button, st_slider, st_checkbox, st_text_area, st_text_input, \
    st_multiselect, st_warning, st_markdown, st_latex, st_table, st_dataframe, st_sidebar_slider, st_file_uploader, st_expander,st_subsubheader,set_page_title_with_image
from Documents_Processing.Web_Process import process_wos_page_upload, process_database_page, process_wos_page_download, process_Overall_Information_Overview_Tab, \
    process_Journay_Analysis_Tab, process_Publication_Author_Tab_Most_Productive, project_root
from Documents_Processing.Tab_Process import process_tabledescribe_tab,process_Publication_Author_Analysis_Tab,process_Publication_Author_Tab_Most_Productive,process_Overall_Information_Overview_Tab,process_Journay_Analysis_Tab,process_Country_Analysis_Tab
from Documents_Processing.Report_Generator import create_report_generator_tab
from Result_Visualization.Network_Visualization import draw_network_visualization,draw_author_network

#â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”path
current_dir = os.path.dirname(os.path.realpath(__file__))
project_root = os.path.dirname(current_dir).replace("/Program","")
library_dir=project_root+ '/library/web_sources/'
picture_path=project_root+ '/output/picture'
csv_path= project_root+'/output/'
audio_file=library_dir+"guita_bgm.mp3"
icon_file_1=library_dir+"å¥¥ç‰¹æ›¼1.png"
icon_file_2=library_dir+'å¥¥ç‰¹æ›¼2.png'
icon_file_3=library_dir+'å¥¥ç‰¹æ›¼3.png'
icon_file_4=library_dir+'å¥¥ç‰¹æ›¼4.png'
css_file=library_dir+"style.css"
#______________________________________________________________________________________________________________________ è®¾ç½®é¡µé¢é…ç½®

# æ•°æ®åŠ è½½ç¼“å­˜
@st.cache_data
def load_data(uploaded_file):
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    if uploaded_file is None:
        return None
    
    try:
        if uploaded_file.name.endswith('.txt'):
            df = parse_txt_with_llm(uploaded_file)
        elif uploaded_file.name.endswith('.csv'):
            df = Load_CSV(uploaded_file)
        elif uploaded_file.name.endswith('.bib'):
            # ä½¿ç”¨å¢å¼ºçš„æ–‡ä»¶ä¸Šä¼ å™¨åŠ è½½BIBæ–‡ä»¶
            try:
                from Documents_Processing.Enhanced_File_Uploader import load_file_universal
                df = load_file_universal(uploaded_file)
            except ImportError:
                st.error("BIBæ–‡ä»¶è§£æåŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æ¨¡å—å¯¼å…¥")
                return None
        else:
            st.error("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼ .txtã€.csvæˆ–.bibæ–‡ä»¶")
            return None
        
        if df is None or df.empty:
            st.warning("æ–‡ä»¶è§£æå¤±è´¥æˆ–æ–‡ä»¶ä¸ºç©º")
            return None
        
        # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
        df = clean_and_standardize_data(df)
        return df
    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        return None

def parse_wos_file(uploaded_file):
    """
    è§£æWeb of Science (WOS) æ ¼å¼çš„TXTæ–‡ä»¶
    WOSæ–‡ä»¶æ ¼å¼ï¼šæ¯è¡Œä»¥2-3ä¸ªå­—ç¬¦çš„å­—æ®µä»£ç å¼€å¤´ï¼Œåè·Ÿç©ºæ ¼å’Œå†…å®¹
    """
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            data = uploaded_file.read().decode("utf-8")
        except UnicodeDecodeError:
            try:
                uploaded_file.seek(0)
                data = uploaded_file.read().decode("utf-8-sig")
            except UnicodeDecodeError:
                try:
                    uploaded_file.seek(0)
                    data = uploaded_file.read().decode("latin-1")
                except UnicodeDecodeError:
                    uploaded_file.seek(0)
                    data = uploaded_file.read().decode("cp1252")
        
        # è§£æWOSæ ¼å¼
        records = []
        current_record = {}
        
        lines = data.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æŸ¥è®°å½•åˆ†éš”ç¬¦
            if line == 'ER':
                # è®°å½•ç»“æŸï¼Œä¿å­˜å½“å‰è®°å½•
                if current_record:
                    records.append(current_record)
                    current_record = {}
                continue
            elif line == 'EF':
                # æ–‡ä»¶ç»“æŸ
                if current_record:
                    records.append(current_record)
                break
            
            # è§£æå­—æ®µ - åªå¤„ç†æ ‡å‡†çš„WOSå­—æ®µä»£ç 
            if len(line) >= 3 and line[2] == ' ':
                field_code = line[:2]
                field_value = line[3:].strip()
                
                # åªå¤„ç†æœ‰æ•ˆçš„WOSå­—æ®µä»£ç 
                valid_fields = {
                    'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB', 'C1', 'RP', 'EM', 'RI', 
                    'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU', 'PI', 'PA', 'SN', 'EI', 'J9', 'JI', 
                    'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'DI', 'PG', 'WC', 'SC', 'GA', 'UT', 'PM', 
                    'DA', 'OA', 'AR', 'FU', 'FX', 'BN', 'BS', 'BA', 'BF', 'BE', 'HO', 'CT', 'CY', 
                    'CL', 'SP', 'SU', 'TA', 'PS', 'PN', 'MA', 'GE', 'UN', 'SE', 'SI', 'CA', 'WE',
                    'VR', 'PT', 'C3', 'OI', 'EA', 'HC', 'HP', 'DB', 'D2', 'PG', 'WC', 'SC', 'GA',
                    'UT', 'PM', 'DA', 'OA', 'AR', 'FU', 'FX', 'BN', 'BS', 'BA', 'BF', 'BE', 'HO',
                    'CT', 'CY', 'CL', 'SP', 'SU', 'TA', 'PS', 'PN', 'MA', 'GE', 'UN', 'SE', 'SI',
                    'CA', 'WE', 'VR', 'PT', 'C3', 'OI', 'EA', 'HC', 'HP', 'DB', 'D2'
                }
                
                if field_code in valid_fields:
                    # å¤„ç†å¤šè¡Œå­—æ®µï¼ˆå¦‚ä½œè€…ã€åœ°å€ç­‰ï¼‰
                    if field_code in current_record:
                        current_record[field_code] += '; ' + field_value
                    else:
                        current_record[field_code] = field_value
        
        if not records:
            st.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡çŒ®è®°å½•")
            return pd.DataFrame()
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(records)
        
        # æ˜¾ç¤ºè§£æç»“æœ
        st.info(f"âœ… æˆåŠŸè§£æ {len(df)} æ¡æ–‡çŒ®è®°å½•")
        st.info(f"ğŸ“‹ è¯†åˆ«åˆ°å­—æ®µ: {', '.join(df.columns.tolist())}")
        
        return df
        
    except Exception as e:
        st.error(f"WOSæ–‡ä»¶è§£æå¤±è´¥: {str(e)}")
        return pd.DataFrame()

def parse_txt_with_llm(uploaded_file):
    """ä½¿ç”¨WOSä¸“ç”¨è§£æå‡½æ•°è§£æTXTæ–‡ä»¶"""
    return parse_wos_file(uploaded_file)

def clean_and_standardize_data(df):
    """æ¸…æ´—å’Œæ ‡å‡†åŒ–æ•°æ®"""
    # é¦–å…ˆå¤„ç†é‡å¤åˆ—å
    if df.columns.duplicated().any():
        # é‡å‘½åé‡å¤çš„åˆ—
        new_columns = []
        seen_columns = {}
        for col in df.columns:
            if col in seen_columns:
                seen_columns[col] += 1
                new_columns.append(f"{col}_{seen_columns[col]}")
            else:
                seen_columns[col] = 0
                new_columns.append(col)
        df.columns = new_columns
    
    # æ ‡å‡†åŒ–åˆ—å - æ˜ å°„WOSæ ‡å‡†å­—æ®µåˆ°åˆ†æç”¨åˆ—å
    column_mapping = {
        # æ ¸å¿ƒå­—æ®µ
        'AU': 'Authors', 'AF': 'Authors',  # ä½œè€…
        'TI': 'Title',  # æ ‡é¢˜
        'SO': 'Source',  # æœŸåˆŠ/æ¥æº
        'PY': 'Year',  # å¹´ä»½
        'AB': 'Abstract',  # æ‘˜è¦
        'DE': 'Keywords',  # å…³é”®è¯
        'ID': 'KeywordsPlus',  # æ‰©å±•å…³é”®è¯
        'C1': 'Address',  # åœ°å€
        'CR': 'References',  # å‚è€ƒæ–‡çŒ®
        'TC': 'TimesCited',  # è¢«å¼•æ¬¡æ•°
        'Z9': 'TotalTimesCited',  # æ€»è¢«å¼•æ¬¡æ•°
        
        # æœŸåˆŠä¿¡æ¯
        'J9': 'JournalAbbreviation',  # æœŸåˆŠç¼©å†™
        'JI': 'JournalISO',  # ISOæœŸåˆŠç¼©å†™
        'VL': 'Volume',  # å·
        'IS': 'Issue',  # æœŸ
        'BP': 'BeginningPage',  # èµ·å§‹é¡µ
        'EP': 'EndingPage',  # ç»“æŸé¡µ
        'DI': 'DOI',  # DOI
        'SN': 'ISSN',  # ISSN
        'EI': 'eISSN',  # eISSN
        
        # ä½œè€…ä¿¡æ¯
        'RP': 'ReprintAddress',  # é‡å°åœ°å€
        'EM': 'EmailAddresses',  # é‚®ç®±
        'RI': 'ResearcherID',  # ç ”ç©¶å‘˜ID
        'OI': 'ORCID',  # ORCID
        
        # åˆ†ç±»ä¿¡æ¯
        'WC': 'WebOfScienceCategory',  # WOSåˆ†ç±»
        'SC': 'SubjectCategory',  # å­¦ç§‘åˆ†ç±»
        'LA': 'Language',  # è¯­è¨€
        'DT': 'DocumentType',  # æ–‡æ¡£ç±»å‹
        'PT': 'PublicationType',  # å‡ºç‰ˆç±»å‹
        
        # å‡ºç‰ˆå•†ä¿¡æ¯
        'PU': 'Publisher',  # å‡ºç‰ˆå•†
        'PI': 'PublisherCity',  # å‡ºç‰ˆå•†åŸå¸‚
        'PA': 'PublisherAddress',  # å‡ºç‰ˆå•†åœ°å€
        
        # å…¶ä»–ä¿¡æ¯
        'UT': 'AccessionNumber',  # å…¥è—å·
        'PM': 'PubMedID',  # PubMed ID
        'AR': 'ArticleNumber',  # æ–‡ç« ç¼–å·
        'PG': 'PageCount',  # é¡µæ•°
        'PD': 'PublicationDate',  # å‘è¡¨æ—¥æœŸ
        
        # èµ„åŠ©ä¿¡æ¯
        'FU': 'FundingAgency',  # èµ„åŠ©æœºæ„
        'FX': 'FundingText',  # èµ„åŠ©æ–‡æœ¬
        
        # ä½¿ç”¨ç»Ÿè®¡
        'U1': 'UsageCount180',  # 180å¤©ä½¿ç”¨æ¬¡æ•°
        'U2': 'UsageCountSince2013',  # 2013å¹´ä»¥æ¥ä½¿ç”¨æ¬¡æ•°
        
        # ä¼šè®®ä¿¡æ¯
        'CT': 'ConferenceTitle',  # ä¼šè®®æ ‡é¢˜
        'CY': 'ConferenceDate',  # ä¼šè®®æ—¥æœŸ
        'CL': 'ConferenceLocation',  # ä¼šè®®åœ°ç‚¹
        'HO': 'ConferenceHost',  # ä¼šè®®ä¸»åŠæ–¹
        
        # ä¹¦ç±ä¿¡æ¯
        'BN': 'ISBN',  # ISBN
        'BA': 'BookAuthors',  # ä¹¦ç±ä½œè€…
        'BE': 'Editors',  # ç¼–è¾‘
        'TA': 'BookTitle',  # ä¹¦ç±æ ‡é¢˜
        
        # å…¶ä»–
        'DA': 'DateAdded',  # æ·»åŠ æ—¥æœŸ
        'OA': 'OpenAccess',  # å¼€æ”¾è·å–
        'HC': 'HighlyCited',  # é«˜è¢«å¼•
        'HP': 'HotPaper',  # çƒ­ç‚¹è®ºæ–‡
    }
    
    # é‡å‘½ååˆ— - åªé‡å‘½åå­˜åœ¨çš„åˆ—
    for old_col, new_col in column_mapping.items():
        if old_col in df.columns:
            df = df.rename(columns={old_col: new_col})
    
    # åˆå¹¶ç›¸åŒåŠŸèƒ½çš„åˆ—
    # åˆå¹¶ä½œè€…å…¨ååˆ°ä½œè€…åˆ—
    if 'AF' in df.columns and 'Authors' in df.columns:
        # å°†AFåˆ—åˆå¹¶åˆ°Authorsåˆ—
        df['Authors'] = df['Authors'].fillna('') + ';' + df['AF'].fillna('')
        df = df.drop(columns=['AF'])
    elif 'AF' in df.columns:
        df = df.rename(columns={'AF': 'Authors'})
    
    # å¤„ç†é‡å¤çš„Authorsåˆ—
    if df.columns.duplicated().any():
        # æ‰¾åˆ°é‡å¤çš„Authorsåˆ—å¹¶åˆå¹¶
        authors_cols = [col for col in df.columns if col == 'Authors']
        if len(authors_cols) > 1:
            # åˆå¹¶æ‰€æœ‰Authorsåˆ—
            df['Authors'] = df[authors_cols].apply(lambda x: ';'.join(x.dropna().astype(str)), axis=1)
            # åˆ é™¤é‡å¤çš„Authorsåˆ—
            df = df.loc[:, ~df.columns.duplicated()]
    
    # åˆå¹¶å…³é”®è¯å’Œæ‰©å±•å…³é”®è¯
    if 'KeywordsPlus' in df.columns and 'Keywords' in df.columns:
        df['Keywords'] = df['Keywords'].fillna('') + ';' + df['KeywordsPlus'].fillna('')
        df = df.drop(columns=['KeywordsPlus'])
    elif 'KeywordsPlus' in df.columns:
        df = df.rename(columns={'KeywordsPlus': 'Keywords'})
    
    # åˆå¹¶è¢«å¼•æ¬¡æ•°å’Œæ€»è¢«å¼•æ¬¡æ•°
    if 'TotalTimesCited' in df.columns and 'TimesCited' in df.columns:
        # å¦‚æœTimesCitedä¸ºç©ºï¼Œä½¿ç”¨TotalTimesCited
        df['TimesCited'] = df['TimesCited'].fillna(df['TotalTimesCited'])
        df = df.drop(columns=['TotalTimesCited'])
    elif 'TotalTimesCited' in df.columns:
        df = df.rename(columns={'TotalTimesCited': 'TimesCited'})
    
    # å¤„ç†å¹´ä»½æ•°æ®
    if 'Year' in df.columns:
        df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
        df = df.dropna(subset=['Year'])
    
    # å¤„ç†å¼•ç”¨æ•°æ®
    if 'TimesCited' in df.columns:
        df['TimesCited'] = pd.to_numeric(df['TimesCited'], errors='coerce').fillna(0)
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    required_columns = ['Authors', 'Source', 'Year']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        st.warning(f"æ–‡ä»¶ç¼ºå°‘å¿…è¦å­—æ®µ: {', '.join(missing_columns)}ï¼Œè¯·æ£€æŸ¥å¯¼å‡ºæ ¼å¼")
        # æ˜¾ç¤ºå¯ç”¨çš„åˆ—åä¾›ç”¨æˆ·å‚è€ƒ
        st.info(f"å½“å‰å¯ç”¨çš„åˆ—å: {', '.join(df.columns.tolist())}")
    
    return df

def safe_get_column(df, possible_names, default_value=""):
    """å®‰å…¨è·å–åˆ—æ•°æ®"""
    for name in possible_names:
        if name in df.columns:
            return df[name]
    return pd.Series([default_value] * len(df))

def create_download_button(data, filename, file_type="csv"):
    """åˆ›å»ºä¸‹è½½æŒ‰é’®"""
    if file_type == "csv":
        csv = data.to_csv(index=False)
        b64 = base64.b64encode(csv.encode()).decode()
        href = f'<a href="data:file/csv;base64,{b64}" download="{filename}.csv">ğŸ“¥ ä¸‹è½½ {filename}.csv</a>'
    elif file_type == "png":
        # å¯¹äºå›¾è¡¨ï¼Œéœ€è¦å…ˆä¿å­˜ä¸ºå›¾ç‰‡
        return None
    return href

# ==================== è®ºæ–‡åˆ†æåŠŸèƒ½æ¨¡å— ====================

def analyze_overview_statistics(df):
    """æ€»ä½“ä¿¡æ¯æ¦‚è§ˆåˆ†æ"""
    st.subheader("ğŸ“Š Overall Information Overview")
    
    # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    total_articles = len(df)
    total_authors = len(set([author for authors in safe_get_column(df, ['Authors']) for author in str(authors).split(';') if author.strip()]))
    total_sources = safe_get_column(df, ['Source']).nunique()
    total_keywords = len(set([kw for keywords in safe_get_column(df, ['Keywords']) for kw in str(keywords).split(';') if kw.strip()]))
    
    # å›½é™…åˆä½œæ¯”ä¾‹
    international_collab = 0
    if 'Address' in df.columns:
        addresses = safe_get_column(df, ['Address'])
        for addr in addresses:
            if pd.notna(addr) and len(str(addr).split(';')) > 1:
                countries = set()
                for country_addr in str(addr).split(';'):
                    if '[' in country_addr and ']' in country_addr:
                        country = country_addr.split('[')[-1].split(']')[0].strip()
                        countries.add(country)
                if len(countries) > 1:
                    international_collab += 1
    
    international_ratio = (international_collab / total_articles * 100) if total_articles > 0 else 0
    
    # å¹³å‡ä½œè€…æ•°
    avg_authors = 0
    if 'Authors' in df.columns:
        author_counts = []
        for authors in safe_get_column(df, ['Authors']):
            if pd.notna(authors):
                author_count = len([a for a in str(authors).split(';') if a.strip()])
                author_counts.append(author_count)
        avg_authors = np.mean(author_counts) if author_counts else 0
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Articles", f"{total_articles:,}")
    with col2:
        st.metric("Total Authors", f"{total_authors:,}")
    with col3:
        st.metric("Total Sources", f"{total_sources:,}")
    with col4:
        st.metric("Total Keywords", f"{total_keywords:,}")
    
    col5, col6 = st.columns(2)
    with col5:
        st.metric("International Collaboration", f"{international_ratio:.1f}%")
    with col6:
        st.metric("Average Authors per Article", f"{avg_authors:.1f}")
    
    # å¹´åº¦å‘æ–‡é‡æŠ˜çº¿å›¾
    if 'Year' in df.columns:
        st.subheader("ğŸ“ˆ Annual Publication Trends")
        year_counts = df['Year'].value_counts().sort_index()
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=year_counts.index,
            y=year_counts.values,
            mode='lines+markers',
            name='Publications',
            line=dict(color='#B5A8CA', width=3),
            marker=dict(size=8, color='#C0D6EA')
        ))
        
        fig.update_layout(
            title="Annual Publication Trends",
            xaxis_title="Year",
            yaxis_title="Number of Publications",
            template="plotly_white",
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # å¯¼å‡ºæŒ‰é’®
        if st.button("ğŸ“¥ Export Annual Trends Data"):
            csv = year_counts.reset_index()
            csv.columns = ['Year', 'Publications']
            st.download_button(
                label="Download CSV",
                data=csv.to_csv(index=False),
                file_name="annual_trends.csv",
                mime="text/csv"
            )

def analyze_authors(df):
    """ä½œè€…åˆ†æ"""
    st.subheader("ğŸ‘¥ Author Analysis")
    
    if 'Authors' not in df.columns:
        st.warning("æœªæ‰¾åˆ°ä½œè€…ä¿¡æ¯åˆ—")
        return
    
    # æ·»åŠ ç­›é€‰é€‰é¡¹
    st.markdown("### ğŸ”§ ç­›é€‰é€‰é¡¹")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        min_publications = st.number_input(
            "æœ€å°å‘è¡¨æ•°é‡", 
            min_value=1, 
            value=1, 
            help="åªæ˜¾ç¤ºå‘è¡¨æ•°é‡å¤§äºç­‰äºæ­¤å€¼çš„ä½œè€…"
        )
    
    with col2:
        min_citations = st.number_input(
            "æœ€å°è¢«å¼•æ¬¡æ•°", 
            min_value=0, 
            value=0, 
            help="åªæ˜¾ç¤ºæ€»è¢«å¼•æ¬¡æ•°å¤§äºç­‰äºæ­¤å€¼çš„ä½œè€…"
        )
    
    with col3:
        top_n_authors = st.number_input(
            "æ˜¾ç¤ºå‰Nä½ä½œè€…", 
            min_value=5, 
            max_value=50, 
            value=15, 
            help="åœ¨å›¾è¡¨ä¸­æ˜¾ç¤ºå‰Nä½ä½œè€…"
        )
    
    # å¤„ç†ä½œè€…æ•°æ®
    all_authors = []
    for authors in safe_get_column(df, ['Authors']):
        if pd.notna(authors):
            for author in str(authors).split(';'):
                author = author.strip()
                if author:
                    all_authors.append(author)
    
    if not all_authors:
        st.warning("æš‚æ— ä½œè€…æ•°æ®")
        return
    
    # ä½œè€…å‘æ–‡é‡ç»Ÿè®¡
    author_counts = pd.Series(all_authors).value_counts()
    
    # åº”ç”¨å‘è¡¨æ•°é‡ç­›é€‰
    filtered_author_counts = author_counts[author_counts >= min_publications]
    
    # æ™®èµ–æ–¯å®šå¾‹è®¡ç®—æ ¸å¿ƒä½œè€…
    total_authors = len(filtered_author_counts)
    price_threshold = 0.749 * np.sqrt(filtered_author_counts.max()) if filtered_author_counts.max() > 0 else 0
    core_authors = filtered_author_counts[filtered_author_counts >= price_threshold]
    
    st.subheader("ğŸ”¬ Core Authors (Price's Law)")
    st.info(f"æ™®èµ–æ–¯å®šå¾‹é˜ˆå€¼: {price_threshold:.1f} ç¯‡")
    
    # æ˜¾ç¤ºæ ¸å¿ƒä½œè€…
    if not core_authors.empty:
        core_df = core_authors.reset_index()
        core_df.columns = ['Author', 'Publications']
        core_df['Rank'] = range(1, len(core_df) + 1)
        
        st.dataframe(core_df, use_container_width=True)
        
        # å¯¼å‡ºæŒ‰é’®
        if st.button("ğŸ“¥ Export Core Authors", key="core_authors_export"):
            st.download_button(
                label="Download CSV",
                data=core_df.to_csv(index=False),
                file_name="core_authors.csv",
                mime="text/csv"
            )
    else:
        st.warning("æœªæ‰¾åˆ°æ ¸å¿ƒä½œè€…")
    
    # é«˜è¢«å¼•ä½œè€…
    st.subheader(f"â­ Highly Cited Authors (Top {top_n_authors})")
    if 'TimesCited' in df.columns:
        # è®¡ç®—æ¯ä¸ªä½œè€…çš„æ€»è¢«å¼•æ¬¡æ•°
        author_citations = defaultdict(int)
        for idx, row in df.iterrows():
            if pd.notna(row.get('Authors')) and pd.notna(row.get('TimesCited')):
                citations = int(row['TimesCited']) if str(row['TimesCited']).isdigit() else 0
                for author in str(row['Authors']).split(';'):
                    author = author.strip()
                    if author:
                        author_citations[author] += citations
        
        # åº”ç”¨è¢«å¼•æ¬¡æ•°ç­›é€‰
        filtered_author_citations = {author: citations for author, citations in author_citations.items() 
                                   if citations >= min_citations}
        
        # æ’åºå¹¶æ˜¾ç¤ºå‰Nä½
        top_cited = sorted(filtered_author_citations.items(), key=lambda x: x[1], reverse=True)[:top_n_authors]
        if top_cited:
            cited_df = pd.DataFrame(top_cited, columns=['Author', 'Total Citations'])
            cited_df['Rank'] = range(1, len(cited_df) + 1)
            
            st.dataframe(cited_df, use_container_width=True)
            
            # å¯¼å‡ºæŒ‰é’®
            if st.button("ğŸ“¥ Export Highly Cited Authors", key="cited_authors_export"):
                st.download_button(
                    label="Download CSV",
                    data=cited_df.to_csv(index=False),
                    file_name="highly_cited_authors.csv",
                    mime="text/csv"
                )
    else:
        st.warning("æœªæ‰¾åˆ°å¼•ç”¨æ•°æ®ï¼Œè¯·æ£€æŸ¥TimesCitedåˆ—æ˜¯å¦å­˜åœ¨")
    
    # ä½œè€…åˆä½œç½‘ç»œå›¾
    st.subheader("ğŸ•¸ï¸ Author Collaboration Network")
    if len(all_authors) > 1:
        # åˆ›å»ºåˆä½œç½‘ç»œ
        G = nx.Graph()
        
        # åªæ·»åŠ æ»¡è¶³ç­›é€‰æ¡ä»¶çš„ä½œè€…ä½œä¸ºèŠ‚ç‚¹
        filtered_authors = set([author for author in all_authors 
                               if author_counts.get(author, 0) >= min_publications])
        
        if len(filtered_authors) < 2:
            st.warning(f"æ»¡è¶³ç­›é€‰æ¡ä»¶çš„ä½œè€…æ•°é‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘2ä½ï¼Œå½“å‰{len(filtered_authors)}ä½ï¼‰ï¼Œæ— æ³•ç»˜åˆ¶åˆä½œç½‘ç»œå›¾")
            return
        
        # æ·»åŠ èŠ‚ç‚¹
        for author in filtered_authors:
            G.add_node(author)
        
        # æ·»åŠ è¾¹ï¼ˆåŒä¸€ç¯‡æ–‡ç« çš„ä½œè€…ä¹‹é—´å»ºç«‹è¿æ¥ï¼‰
        for authors in safe_get_column(df, ['Authors']):
            if pd.notna(authors):
                author_list = [a.strip() for a in str(authors).split(';') if a.strip()]
                for i in range(len(author_list)):
                    for j in range(i+1, len(author_list)):
                        if G.has_edge(author_list[i], author_list[j]):
                            G[author_list[i]][author_list[j]]['weight'] += 1
                        else:
                            G.add_edge(author_list[i], author_list[j], weight=1)
        
        if G.number_of_edges() > 0:
            # ä½¿ç”¨springå¸ƒå±€
            pos = nx.spring_layout(G, k=1, iterations=50)
            
            # åˆ›å»ºç½‘ç»œå›¾
            edge_trace = []
            for edge in G.edges():
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                weight = G[edge[0]][edge[1]]['weight']
                edge_trace.append(go.Scatter(
                    x=[x0, x1, None],
                    y=[y0, y1, None],
                    mode='lines',
                    line=dict(width=weight*0.5, color='#888'),
                    hoverinfo='none'
                ))
            
            # èŠ‚ç‚¹å¤§å°åŸºäºå‘æ–‡é‡
            node_sizes = [author_counts[node] * 10 for node in G.nodes()]
            
            node_trace = go.Scatter(
                x=[pos[node][0] for node in G.nodes()],
                y=[pos[node][1] for node in G.nodes()],
                mode='markers+text',
                text=[node[:15] + '...' if len(node) > 15 else node for node in G.nodes()],
                textposition="middle center",
                hoverinfo='text',
                marker=dict(
                    size=node_sizes,
                    color='#B5A8CA',
                    line=dict(width=2, color='#C0D6EA')
                )
            )
            
            fig = go.Figure(data=edge_trace + [node_trace])
            fig.update_layout(
                title="Author Collaboration Network",
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                height=600
            )
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("ä½œè€…åˆä½œç½‘ç»œæ•°æ®ä¸è¶³")
    
    # è¢«å¼•è€¦åˆç½‘ç»œå›¾
    st.subheader("ğŸ”— Citation Coupling Network")
    if 'References' in df.columns and len(all_authors) > 1:
        # åˆ›å»ºè¢«å¼•è€¦åˆç½‘ç»œ
        G_coupling = nx.Graph()
        
        # ä¸ºæ¯ä¸ªä½œè€…æ”¶é›†å…¶å¼•ç”¨çš„æ–‡çŒ®
        author_refs = defaultdict(set)
        for idx, row in df.iterrows():
            if pd.notna(row.get('Authors')) and pd.notna(row.get('References')):
                for author in str(row['Authors']).split(';'):
                    author = author.strip()
                    if author:
                        for ref in str(row['References']).split(';'):
                            ref = ref.strip()
                            if ref:
                                author_refs[author].add(ref)
        
        # è®¡ç®—ä½œè€…é—´çš„å¼•ç”¨è€¦åˆå¼ºåº¦
        authors_list = list(author_refs.keys())
        for i in range(len(authors_list)):
            for j in range(i+1, len(authors_list)):
                author1, author2 = authors_list[i], authors_list[j]
                refs1, refs2 = author_refs[author1], author_refs[author2]
                
                # è®¡ç®—å…±åŒå¼•ç”¨çš„æ–‡çŒ®æ•°é‡
                common_refs = len(refs1.intersection(refs2))
                if common_refs > 0:
                    G_coupling.add_edge(author1, author2, weight=common_refs)
        
        if G_coupling.number_of_edges() > 0:
            # ä½¿ç”¨springå¸ƒå±€
            pos = nx.spring_layout(G_coupling, k=1, iterations=50)
            
            # åˆ›å»ºç½‘ç»œå›¾
            edge_trace = []
            for edge in G_coupling.edges():
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                weight = G_coupling[edge[0]][edge[1]]['weight']
                edge_trace.append(go.Scatter(
                    x=[x0, x1, None],
                    y=[y0, y1, None],
                    mode='lines',
                    line=dict(width=weight*0.3, color='#FF6B6B'),
                    hoverinfo='none'
                ))
            
            node_trace = go.Scatter(
                x=[pos[node][0] for node in G_coupling.nodes()],
                y=[pos[node][1] for node in G_coupling.nodes()],
                mode='markers+text',
                text=[node[:15] + '...' if len(node) > 15 else node for node in G_coupling.nodes()],
                textposition="middle center",
                hoverinfo='text',
                marker=dict(
                    size=20,
                    color='#FF6B6B',
                    line=dict(width=2, color='#FF8E8E')
                )
            )
            
            fig = go.Figure(data=edge_trace + [node_trace])
            fig.update_layout(
                title="Citation Coupling Network",
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                height=600
            )
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("è¢«å¼•è€¦åˆç½‘ç»œæ•°æ®ä¸è¶³")
    else:
        st.warning("éœ€è¦å‚è€ƒæ–‡çŒ®æ•°æ®æ¥æ„å»ºè¢«å¼•è€¦åˆç½‘ç»œ")

def analyze_countries(df):
    """å›½å®¶ä¸åœ°åŒºåˆ†æ"""
    st.subheader("ğŸŒ Country and Region Analysis")
    
    if 'Address' not in df.columns:
        st.warning("æœªæ‰¾åˆ°åœ°å€ä¿¡æ¯åˆ—")
        return
    
    # æ·»åŠ ç­›é€‰é€‰é¡¹
    st.markdown("### ğŸ”§ ç­›é€‰é€‰é¡¹")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        min_publications_country = st.number_input(
            "æœ€å°å‘æ–‡æ•°é‡", 
            min_value=1, 
            value=1, 
            help="åªæ˜¾ç¤ºå‘æ–‡æ•°é‡å¤§äºç­‰äºæ­¤å€¼çš„å›½å®¶",
            key="country_min_pub"
        )
    
    with col2:
        min_citations_country = st.number_input(
            "æœ€å°è¢«å¼•æ¬¡æ•°", 
            min_value=0, 
            value=0, 
            help="åªæ˜¾ç¤ºæ€»è¢«å¼•æ¬¡æ•°å¤§äºç­‰äºæ­¤å€¼çš„å›½å®¶",
            key="country_min_cit"
        )
    
    with col3:
        top_n_countries = st.number_input(
            "æ˜¾ç¤ºå‰Nä¸ªå›½å®¶", 
            min_value=5, 
            max_value=30, 
            value=10, 
            help="åœ¨å›¾è¡¨ä¸­æ˜¾ç¤ºå‰Nä¸ªå›½å®¶",
            key="country_top_n"
        )
    
    # æå–å›½å®¶ä¿¡æ¯
    countries = []
    for addr in safe_get_column(df, ['Address']):
        if pd.notna(addr):
            for country_addr in str(addr).split(';'):
                if '[' in country_addr and ']' in country_addr:
                    country = country_addr.split('[')[-1].split(']')[0].strip()
                    if country:
                        countries.append(country)
    
    if not countries:
        st.warning("æœªæ‰¾åˆ°å›½å®¶ä¿¡æ¯")
        return
    
    # å›½å®¶å‘æ–‡é‡ç»Ÿè®¡
    country_counts = pd.Series(countries).value_counts()
    
    # å›½å®¶å¼•ç”¨é‡ç»Ÿè®¡
    country_citations = defaultdict(int)
    for idx, row in df.iterrows():
        if pd.notna(row.get('Address')) and pd.notna(row.get('TimesCited')):
            citations = int(row['TimesCited']) if str(row['TimesCited']).isdigit() else 0
            for country_addr in str(row['Address']).split(';'):
                if '[' in country_addr and ']' in country_addr:
                    country = country_addr.split('[')[-1].split(']')[0].strip()
                    if country:
                        country_citations[country] += citations
    
    # åº”ç”¨ç­›é€‰æ¡ä»¶
    filtered_countries = []
    for country in country_counts.index:
        pub_count = country_counts[country]
        cit_count = country_citations.get(country, 0)
        if pub_count >= min_publications_country and cit_count >= min_citations_country:
            filtered_countries.append(country)
    
    if not filtered_countries:
        st.warning(f"æ²¡æœ‰å›½å®¶æ»¡è¶³ç­›é€‰æ¡ä»¶ï¼ˆå‘æ–‡æ•°é‡â‰¥{min_publications_country}ï¼Œè¢«å¼•æ¬¡æ•°â‰¥{min_citations_country}ï¼‰")
        return
    
    # æ˜¾ç¤ºå›½å®¶ç»Ÿè®¡è¡¨æ ¼
    st.subheader("ğŸ“Š Country Statistics")
    filtered_country_counts = country_counts[filtered_countries]
    country_stats = pd.DataFrame({
        'Country': filtered_country_counts.index,
        'Publications': filtered_country_counts.values,
        'Citations': [country_citations.get(country, 0) for country in filtered_country_counts.index]
    })
    country_stats['Citations per Paper'] = country_stats['Citations'] / country_stats['Publications']
    country_stats = country_stats.sort_values('Publications', ascending=False)
    country_stats['Rank'] = range(1, len(country_stats) + 1)
    
    st.dataframe(country_stats.head(top_n_countries), use_container_width=True)
    
    # å¯¼å‡ºæŒ‰é’®
    if st.button("ğŸ“¥ Export Country Statistics", key="country_stats_export"):
        st.download_button(
            label="Download CSV",
            data=country_stats.to_csv(index=False),
            file_name="country_statistics.csv",
            mime="text/csv"
        )
    
    # å›½å®¶åˆä½œåœ°å›¾
    st.subheader("ğŸ—ºï¸ Country Collaboration Map")
    if len(filtered_countries) > 1:
        # åˆ›å»ºå›½å®¶åˆä½œç½‘ç»œ
        G_country = nx.Graph()
        
        # åªæ·»åŠ æ»¡è¶³ç­›é€‰æ¡ä»¶çš„å›½å®¶ä½œä¸ºèŠ‚ç‚¹
        for country in filtered_countries:
            G_country.add_node(country)
        
        # æ·»åŠ è¾¹ï¼ˆåŒä¸€ç¯‡æ–‡ç« çš„å›½å®¶ä¹‹é—´å»ºç«‹è¿æ¥ï¼‰
        for addr in safe_get_column(df, ['Address']):
            if pd.notna(addr):
                country_list = []
                for country_addr in str(addr).split(';'):
                    if '[' in country_addr and ']' in country_addr:
                        country = country_addr.split('[')[-1].split(']')[0].strip()
                        if country:
                            country_list.append(country)
                
                for i in range(len(country_list)):
                    for j in range(i+1, len(country_list)):
                        if G_country.has_edge(country_list[i], country_list[j]):
                            G_country[country_list[i]][country_list[j]]['weight'] += 1
                        else:
                            G_country.add_edge(country_list[i], country_list[j], weight=1)
        
        if G_country.number_of_edges() > 0:
            # ä½¿ç”¨springå¸ƒå±€
            pos = nx.spring_layout(G_country, k=1, iterations=50)
            
            # åˆ›å»ºç½‘ç»œå›¾
            edge_trace = []
            for edge in G_country.edges():
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                weight = G_country[edge[0]][edge[1]]['weight']
                edge_trace.append(go.Scatter(
                    x=[x0, x1, None],
                    y=[y0, y1, None],
                    mode='lines',
                    line=dict(width=weight*0.5, color='#4ECDC4'),
                    hoverinfo='none'
                ))
            
            # èŠ‚ç‚¹å¤§å°åŸºäºå‘æ–‡é‡
            node_sizes = [country_counts[node] * 15 for node in G_country.nodes()]
            
            node_trace = go.Scatter(
                x=[pos[node][0] for node in G_country.nodes()],
                y=[pos[node][1] for node in G_country.nodes()],
                mode='markers+text',
                text=list(G_country.nodes()),
                textposition="middle center",
                hoverinfo='text',
                marker=dict(
                    size=node_sizes,
                    color='#4ECDC4',
                    line=dict(width=2, color='#45B7D1')
                )
            )
            
            fig = go.Figure(data=edge_trace + [node_trace])
            fig.update_layout(
                title="Country Collaboration Network",
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                height=600
            )
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("å›½å®¶åˆä½œç½‘ç»œæ•°æ®ä¸è¶³")
    
    # å¹´åº¦å‘æ–‡è¶‹åŠ¿ï¼ˆä¸»è¦å›½å®¶ï¼‰
    st.subheader("ğŸ“ˆ Annual Publication Trends by Country")
    if 'Year' in df.columns:
        # é€‰æ‹©å‘æ–‡é‡å‰5çš„å›½å®¶
        top_countries = country_counts.head(5).index.tolist()
        
        # æŒ‰å¹´ä»½å’Œå›½å®¶ç»Ÿè®¡
        yearly_country_data = defaultdict(lambda: defaultdict(int))
        for idx, row in df.iterrows():
            if pd.notna(row.get('Year')) and pd.notna(row.get('Address')):
                year = int(row['Year'])
                for country_addr in str(row['Address']).split(';'):
                    if '[' in country_addr and ']' in country_addr:
                        country = country_addr.split('[')[-1].split(']')[0].strip()
                        if country in top_countries:
                            yearly_country_data[year][country] += 1
        
        # åˆ›å»ºæŠ˜çº¿å›¾
        fig = go.Figure()
        colors = ['#B5A8CA', '#C0D6EA', '#E0BBD0', '#FF6B6B', '#4ECDC4']
        
        for i, country in enumerate(top_countries):
            years = sorted(yearly_country_data.keys())
            counts = [yearly_country_data[year].get(country, 0) for year in years]
            
            fig.add_trace(go.Scatter(
                x=years,
                y=counts,
                mode='lines+markers',
                name=country,
                line=dict(color=colors[i % len(colors)], width=3),
                marker=dict(size=8)
            ))
        
        fig.update_layout(
            title="Annual Publication Trends by Top Countries",
            xaxis_title="Year",
            yaxis_title="Number of Publications",
            template="plotly_white",
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # å¯¼å‡ºæŒ‰é’®
        if st.button("ğŸ“¥ Export Country Trends", key="country_trends_export"):
            # å‡†å¤‡å¯¼å‡ºæ•°æ®
            export_data = []
            for country in top_countries:
                years = sorted(yearly_country_data.keys())
                for year in years:
                    count = yearly_country_data[year].get(country, 0)
                    export_data.append({'Country': country, 'Year': year, 'Publications': count})
            
            export_df = pd.DataFrame(export_data)
            st.download_button(
                label="Download CSV",
                data=export_df.to_csv(index=False),
                file_name="country_trends.csv",
                mime="text/csv"
            )

def analyze_institutions(df):
    """æœºæ„åˆ†æ"""
    st.subheader("ğŸ›ï¸ Institution Analysis")
    
    if 'Address' not in df.columns:
        st.warning("æœªæ‰¾åˆ°åœ°å€ä¿¡æ¯åˆ—")
        return
    
    # æ·»åŠ ç­›é€‰é€‰é¡¹
    st.markdown("### ğŸ”§ ç­›é€‰é€‰é¡¹")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        min_publications_institution = st.number_input(
            "æœ€å°å‘æ–‡æ•°é‡", 
            min_value=1, 
            value=1, 
            help="åªæ˜¾ç¤ºå‘æ–‡æ•°é‡å¤§äºç­‰äºæ­¤å€¼çš„æœºæ„",
            key="institution_min_pub"
        )
    
    with col2:
        min_citations_institution = st.number_input(
            "æœ€å°è¢«å¼•æ¬¡æ•°", 
            min_value=0, 
            value=0, 
            help="åªæ˜¾ç¤ºæ€»è¢«å¼•æ¬¡æ•°å¤§äºç­‰äºæ­¤å€¼çš„æœºæ„",
            key="institution_min_cit"
        )
    
    with col3:
        top_n_institutions = st.number_input(
            "æ˜¾ç¤ºå‰Nä¸ªæœºæ„", 
            min_value=5, 
            max_value=30, 
            value=10, 
            help="åœ¨å›¾è¡¨ä¸­æ˜¾ç¤ºå‰Nä¸ªæœºæ„",
            key="institution_top_n"
        )
    
    # æå–æœºæ„ä¿¡æ¯
    institutions = []
    for addr in safe_get_column(df, ['Address']):
        if pd.notna(addr):
            for institution_addr in str(addr).split(';'):
                # æå–æœºæ„åç§°ï¼ˆåœ¨æ–¹æ‹¬å·å‰ï¼‰
                if '[' in institution_addr:
                    institution = institution_addr.split('[')[0].strip()
                    if institution:
                        institutions.append(institution)
    
    if not institutions:
        st.warning("æœªæ‰¾åˆ°æœºæ„ä¿¡æ¯")
        return
    
    # æœºæ„å‘æ–‡é‡ç»Ÿè®¡
    institution_counts = pd.Series(institutions).value_counts()
    
    # æœºæ„å¼•ç”¨é‡ç»Ÿè®¡
    institution_citations = defaultdict(int)
    for idx, row in df.iterrows():
        if pd.notna(row.get('Address')) and pd.notna(row.get('TimesCited')):
            citations = int(row['TimesCited']) if str(row['TimesCited']).isdigit() else 0
            for institution_addr in str(row['Address']).split(';'):
                if '[' in institution_addr:
                    institution = institution_addr.split('[')[0].strip()
                    if institution:
                        institution_citations[institution] += citations
    
    # åº”ç”¨ç­›é€‰æ¡ä»¶
    filtered_institutions = []
    for institution in institution_counts.index:
        pub_count = institution_counts[institution]
        cit_count = institution_citations.get(institution, 0)
        if pub_count >= min_publications_institution and cit_count >= min_citations_institution:
            filtered_institutions.append(institution)
    
    if not filtered_institutions:
        st.warning(f"æ²¡æœ‰æœºæ„æ»¡è¶³ç­›é€‰æ¡ä»¶ï¼ˆå‘æ–‡æ•°é‡â‰¥{min_publications_institution}ï¼Œè¢«å¼•æ¬¡æ•°â‰¥{min_citations_institution}ï¼‰")
        return
    
    # æ˜¾ç¤ºæœºæ„ç»Ÿè®¡è¡¨æ ¼
    st.subheader("ğŸ“Š Top Institutions by Publications")
    filtered_institution_counts = institution_counts[filtered_institutions]
    institution_stats = pd.DataFrame({
        'Institution': filtered_institution_counts.index,
        'Publications': filtered_institution_counts.values,
        'Citations': [institution_citations.get(institution, 0) for institution in filtered_institution_counts.index]
    })
    institution_stats['Citations per Paper'] = institution_stats['Citations'] / institution_stats['Publications']
    institution_stats = institution_stats.sort_values('Publications', ascending=False)
    institution_stats['Rank'] = range(1, len(institution_stats) + 1)
    
    st.dataframe(institution_stats.head(top_n_institutions), use_container_width=True)
    
    # å¯¼å‡ºæŒ‰é’®
    if st.button("ğŸ“¥ Export Institution Statistics", key="institution_stats_export"):
        st.download_button(
            label="Download CSV",
            data=institution_stats.to_csv(index=False),
            file_name="institution_statistics.csv",
            mime="text/csv"
        )
    
    # æœºæ„åˆä½œç½‘ç»œå›¾
    st.subheader("ğŸ•¸ï¸ Institution Collaboration Network")
    if len(filtered_institutions) > 1:
        # åˆ›å»ºæœºæ„åˆä½œç½‘ç»œ
        G = nx.Graph()
        
        # åªæ·»åŠ æ»¡è¶³ç­›é€‰æ¡ä»¶çš„æœºæ„ä½œä¸ºèŠ‚ç‚¹
        for institution in filtered_institutions:
            G.add_node(institution)
        
        # æ·»åŠ è¾¹ï¼ˆåŒä¸€ç¯‡æ–‡ç« çš„æœºæ„ä¹‹é—´å»ºç«‹è¿æ¥ï¼‰
        for addr in safe_get_column(df, ['Address']):
            if pd.notna(addr):
                institution_list = []
                for institution_addr in str(addr).split(';'):
                    if '[' in institution_addr:
                        institution = institution_addr.split('[')[0].strip()
                        if institution:
                            institution_list.append(institution)
                
                for i in range(len(institution_list)):
                    for j in range(i+1, len(institution_list)):
                        if G.has_edge(institution_list[i], institution_list[j]):
                            G[institution_list[i]][institution_list[j]]['weight'] += 1
                        else:
                            G.add_edge(institution_list[i], institution_list[j], weight=1)
        
        if G.number_of_edges() > 0:
            # ä½¿ç”¨springå¸ƒå±€
            pos = nx.spring_layout(G, k=1, iterations=50)
            
            # åˆ›å»ºç½‘ç»œå›¾
            edge_trace = []
            for edge in G.edges():
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                weight = G[edge[0]][edge[1]]['weight']
                edge_trace.append(go.Scatter(
                    x=[x0, x1, None],
                    y=[y0, y1, None],
                    mode='lines',
                    line=dict(width=weight*0.3, color='#888'),
                    hoverinfo='none'
                ))
            
            # èŠ‚ç‚¹å¤§å°åŸºäºå‘æ–‡é‡
            node_sizes = [institution_counts[node] * 10 for node in G.nodes()]
            
            node_trace = go.Scatter(
                x=[pos[node][0] for node in G.nodes()],
                y=[pos[node][1] for node in G.nodes()],
                mode='markers+text',
                text=[node[:20] + '...' if len(node) > 20 else node for node in G.nodes()],
                textposition="middle center",
                hoverinfo='text',
                marker=dict(
                    size=node_sizes,
                    color='#B5A8CA',
                    line=dict(width=2, color='#C0D6EA')
                )
            )
            
            fig = go.Figure(data=edge_trace + [node_trace])
            fig.update_layout(
                title="Institution Collaboration Network",
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                height=600
            )
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("æœºæ„åˆä½œç½‘ç»œæ•°æ®ä¸è¶³")
    
    # å¹´åº¦äº§å‡ºå˜åŒ–
    st.subheader("ğŸ“ˆ Annual Output Changes by Institution")
    if 'Year' in df.columns:
        # é€‰æ‹©å‘æ–‡é‡å‰10çš„æœºæ„
        top_institutions = institution_counts.head(10).index.tolist()
        
        # æŒ‰å¹´ä»½å’Œæœºæ„ç»Ÿè®¡
        yearly_institution_data = defaultdict(lambda: defaultdict(int))
        for idx, row in df.iterrows():
            if pd.notna(row.get('Year')) and pd.notna(row.get('Address')):
                year = int(row['Year'])
                for institution_addr in str(row['Address']).split(';'):
                    if '[' in institution_addr:
                        institution = institution_addr.split('[')[0].strip()
                        if institution in top_institutions:
                            yearly_institution_data[year][institution] += 1
        
        # åˆ›å»ºæŠ˜çº¿å›¾
        fig = go.Figure()
        colors = ['#B5A8CA', '#C0D6EA', '#E0BBD0', '#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#A8E6CF', '#FFD3A5']
        
        for i, institution in enumerate(top_institutions):
            years = sorted(yearly_institution_data.keys())
            counts = [yearly_institution_data[year].get(institution, 0) for year in years]
            
            fig.add_trace(go.Scatter(
                x=years,
                y=counts,
                mode='lines+markers',
                name=institution[:30] + '...' if len(institution) > 30 else institution,
                line=dict(color=colors[i % len(colors)], width=3),
                marker=dict(size=8)
            ))
        
        fig.update_layout(
            title="Annual Output Changes by Top Institutions",
            xaxis_title="Year",
            yaxis_title="Number of Publications",
            template="plotly_white",
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # å¯¼å‡ºæŒ‰é’®
        if st.button("ğŸ“¥ Export Institution Trends", key="institution_trends_export"):
            # å‡†å¤‡å¯¼å‡ºæ•°æ®
            export_data = []
            for institution in top_institutions:
                years = sorted(yearly_institution_data.keys())
                for year in years:
                    count = yearly_institution_data[year].get(institution, 0)
                    export_data.append({'Institution': institution, 'Year': year, 'Publications': count})
            
            export_df = pd.DataFrame(export_data)
            st.download_button(
                label="Download CSV",
                data=export_df.to_csv(index=False),
                file_name="institution_trends.csv",
                mime="text/csv"
            )

def analyze_cited_references(df):
    """è¢«å¼•æ–‡çŒ®åˆ†æ"""
    st.subheader("ğŸ“š Cited References Analysis")
    
    if 'References' not in df.columns:
        st.warning("æœªæ‰¾åˆ°å‚è€ƒæ–‡çŒ®åˆ—")
        return
    
    # ç»Ÿè®¡è¢«å¼•æ–‡çŒ®
    all_refs = []
    for refs in safe_get_column(df, ['References']):
        if pd.notna(refs):
            for ref in str(refs).split(';'):
                ref = ref.strip()
                if ref:
                    all_refs.append(ref)
    
    if not all_refs:
        st.warning("æš‚æ— å‚è€ƒæ–‡çŒ®æ•°æ®")
        return
    
    # è¢«å¼•æ–‡çŒ®ç»Ÿè®¡
    ref_counts = pd.Series(all_refs).value_counts()
    
    # æ˜¾ç¤ºé«˜è¢«å¼•æ–‡çŒ®è¡¨æ ¼
    st.subheader("â­ Highly Cited References")
    ref_stats = pd.DataFrame({
        'Reference': ref_counts.index,
        'Citation Count': ref_counts.values
    })
    ref_stats['Rank'] = range(1, len(ref_stats) + 1)
    
    st.dataframe(ref_stats.head(20), use_container_width=True)
    
    # å¯¼å‡ºæŒ‰é’®
    if st.button("ğŸ“¥ Export Cited References", key="cited_refs_export"):
        st.download_button(
            label="Download CSV",
            data=ref_stats.to_csv(index=False),
            file_name="cited_references.csv",
            mime="text/csv"
        )
    
    # å…±è¢«å¼•ç½‘ç»œå›¾
    st.subheader("ğŸ•¸ï¸ Co-citation Network")
    if len(all_refs) > 1:
        # é€‰æ‹©é«˜é¢‘è¢«å¼•æ–‡çŒ®
        top_refs = ref_counts.head(30).index.tolist()
        
        # åˆ›å»ºå…±è¢«å¼•ç½‘ç»œ
        G_cocitation = nx.Graph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for ref in top_refs:
            G_cocitation.add_node(ref)
        
        # æ·»åŠ è¾¹ï¼ˆåŒä¸€ç¯‡æ–‡ç« å¼•ç”¨çš„æ–‡çŒ®ä¹‹é—´å»ºç«‹è¿æ¥ï¼‰
        for refs in safe_get_column(df, ['References']):
            if pd.notna(refs):
                ref_list = []
                for ref in str(refs).split(';'):
                    ref = ref.strip()
                    if ref in top_refs:
                        ref_list.append(ref)
                
                for i in range(len(ref_list)):
                    for j in range(i+1, len(ref_list)):
                        if G_cocitation.has_edge(ref_list[i], ref_list[j]):
                            G_cocitation[ref_list[i]][ref_list[j]]['weight'] += 1
                        else:
                            G_cocitation.add_edge(ref_list[i], ref_list[j], weight=1)
        
        if G_cocitation.number_of_edges() > 0:
            # ä½¿ç”¨springå¸ƒå±€
            pos = nx.spring_layout(G_cocitation, k=1, iterations=50)
            
            # åˆ›å»ºç½‘ç»œå›¾
            edge_trace = []
            for edge in G_cocitation.edges():
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                weight = G_cocitation[edge[0]][edge[1]]['weight']
                edge_trace.append(go.Scatter(
                    x=[x0, x1, None],
                    y=[y0, y1, None],
                    mode='lines',
                    line=dict(width=weight*0.3, color='#E0BBD0'),
                    hoverinfo='none'
                ))
            
            # èŠ‚ç‚¹å¤§å°åŸºäºè¢«å¼•æ¬¡æ•°
            node_sizes = [ref_counts[node] * 5 for node in G_cocitation.nodes()]
            
            node_trace = go.Scatter(
                x=[pos[node][0] for node in G_cocitation.nodes()],
                y=[pos[node][1] for node in G_cocitation.nodes()],
                mode='markers+text',
                text=[node[:30] + '...' if len(node) > 30 else node for node in G_cocitation.nodes()],
                textposition="middle center",
                hoverinfo='text',
                marker=dict(
                    size=node_sizes,
                    color='#E0BBD0',
                    line=dict(width=2, color='#FFB6C1')
                )
            )
            
            fig = go.Figure(data=edge_trace + [node_trace])
            fig.update_layout(
                title="Co-citation Network",
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                height=600
            )
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("å…±è¢«å¼•ç½‘ç»œæ•°æ®ä¸è¶³")

def analyze_keywords(df):
    """å…³é”®è¯å…±ç°åˆ†æ"""
    st.subheader("ğŸ”‘ Keywords Co-occurrence Analysis")
    
    if 'Keywords' not in df.columns:
        st.warning("æœªæ‰¾åˆ°å…³é”®è¯åˆ—")
        return
    
    # æ·»åŠ ç­›é€‰é€‰é¡¹
    st.markdown("### ğŸ”§ ç­›é€‰é€‰é¡¹")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        min_keyword_frequency = st.number_input(
            "æœ€å°å…³é”®è¯é¢‘æ¬¡", 
            min_value=1, 
            value=2, 
            help="åªæ˜¾ç¤ºå‡ºç°é¢‘æ¬¡å¤§äºç­‰äºæ­¤å€¼çš„å…³é”®è¯",
            key="keyword_min_freq"
        )
    
    with col2:
        min_cooccurrence = st.number_input(
            "æœ€å°å…±ç°æ¬¡æ•°", 
            min_value=1, 
            value=1, 
            help="åªæ˜¾ç¤ºå…±ç°æ¬¡æ•°å¤§äºç­‰äºæ­¤å€¼çš„å…³é”®è¯å¯¹",
            key="keyword_min_cooc"
        )
    
    with col3:
        top_n_keywords = st.number_input(
            "æ˜¾ç¤ºå‰Nä¸ªå…³é”®è¯", 
            min_value=10, 
            max_value=100, 
            value=30, 
            help="åœ¨å›¾è¡¨ä¸­æ˜¾ç¤ºå‰Nä¸ªå…³é”®è¯",
            key="keyword_top_n"
        )
    
    # æå–å…³é”®è¯
    all_keywords = []
    for keywords in safe_get_column(df, ['Keywords']):
        if pd.notna(keywords):
            for kw in str(keywords).split(';'):
                kw = kw.strip().lower()
                if kw and len(kw) > 2:  # è¿‡æ»¤å¤ªçŸ­çš„å…³é”®è¯
                    all_keywords.append(kw)
    
    if not all_keywords:
        st.warning("æš‚æ— å…³é”®è¯æ•°æ®")
        return
    
    # å…³é”®è¯é¢‘ç‡ç»Ÿè®¡
    kw_counts = pd.Series(all_keywords).value_counts()
    
    # åº”ç”¨é¢‘æ¬¡ç­›é€‰
    filtered_kw_counts = kw_counts[kw_counts >= min_keyword_frequency]
    
    if filtered_kw_counts.empty:
        st.warning(f"æ²¡æœ‰å…³é”®è¯æ»¡è¶³é¢‘æ¬¡ç­›é€‰æ¡ä»¶ï¼ˆâ‰¥{min_keyword_frequency}æ¬¡ï¼‰")
        return
    
    # æ˜¾ç¤ºçƒ­ç‚¹å…³é”®è¯è¡¨æ ¼
    st.subheader("ğŸ”¥ Hot Keywords Frequency")
    kw_stats = pd.DataFrame({
        'Keyword': filtered_kw_counts.index,
        'Frequency': filtered_kw_counts.values
    })
    kw_stats = kw_stats.sort_values('Frequency', ascending=False)
    kw_stats['Rank'] = range(1, len(kw_stats) + 1)
    
    st.dataframe(kw_stats.head(top_n_keywords), use_container_width=True)
    
    # å¯¼å‡ºæŒ‰é’®
    if st.button("ğŸ“¥ Export Keywords", key="keywords_export"):
        st.download_button(
            label="Download CSV",
            data=kw_stats.to_csv(index=False),
            file_name="keywords_frequency.csv",
            mime="text/csv"
        )
    
    # å…³é”®è¯å…±ç°ç½‘ç»œå›¾
    st.subheader("ğŸ•¸ï¸ Keywords Co-occurrence Network")
    if len(filtered_kw_counts) > 1:
        # é€‰æ‹©é«˜é¢‘å…³é”®è¯
        top_keywords = filtered_kw_counts.head(top_n_keywords).index.tolist()
        
        # åˆ›å»ºå…±ç°ç½‘ç»œ
        G = nx.Graph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for kw in top_keywords:
            G.add_node(kw)
        
        # æ·»åŠ è¾¹ï¼ˆåŒä¸€ç¯‡æ–‡ç« çš„å…³é”®è¯ä¹‹é—´å»ºç«‹è¿æ¥ï¼‰
        for keywords in safe_get_column(df, ['Keywords']):
            if pd.notna(keywords):
                kw_list = []
                for kw in str(keywords).split(';'):
                    kw = kw.strip().lower()
                    if kw in top_keywords:
                        kw_list.append(kw)
                
                for i in range(len(kw_list)):
                    for j in range(i+1, len(kw_list)):
                        if G.has_edge(kw_list[i], kw_list[j]):
                            G[kw_list[i]][kw_list[j]]['weight'] += 1
                        else:
                            G.add_edge(kw_list[i], kw_list[j], weight=1)
        
        # åº”ç”¨å…±ç°æ¬¡æ•°ç­›é€‰
        edges_to_remove = []
        for edge in G.edges():
            if G[edge[0]][edge[1]]['weight'] < min_cooccurrence:
                edges_to_remove.append(edge)
        
        for edge in edges_to_remove:
            G.remove_edge(edge[0], edge[1])
        
        if G.number_of_edges() > 0:
            # ä½¿ç”¨springå¸ƒå±€
            pos = nx.spring_layout(G, k=1, iterations=50)
            
            # åˆ›å»ºç½‘ç»œå›¾
            edge_trace = []
            for edge in G.edges():
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                weight = G[edge[0]][edge[1]]['weight']
                edge_trace.append(go.Scatter(
                    x=[x0, x1, None],
                    y=[y0, y1, None],
                    mode='lines',
                    line=dict(width=weight*0.5, color='#888'),
                    hoverinfo='none'
                ))
            
            # èŠ‚ç‚¹å¤§å°åŸºäºé¢‘ç‡
            node_sizes = [filtered_kw_counts[node] * 10 for node in G.nodes()]
            
            node_trace = go.Scatter(
                x=[pos[node][0] for node in G.nodes()],
                y=[pos[node][1] for node in G.nodes()],
                mode='markers+text',
                text=list(G.nodes()),
                textposition="middle center",
                hoverinfo='text',
                marker=dict(
                    size=node_sizes,
                    color='#B5A8CA',
                    line=dict(width=2, color='#C0D6EA')
                )
            )
            
            fig = go.Figure(data=edge_trace + [node_trace])
            fig.update_layout(
                title="Keywords Co-occurrence Network",
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                height=600
            )
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("å…³é”®è¯å…±ç°ç½‘ç»œæ•°æ®ä¸è¶³")

def analyze_trends(df):
    """ç ”ç©¶è¶‹åŠ¿ä¸çƒ­ç‚¹åˆ†æ"""
    st.subheader("ğŸ“ˆ Research Trends and Hot Topics Analysis")
    
    if 'Year' not in df.columns or 'Keywords' not in df.columns:
        st.warning("éœ€è¦å¹´ä»½å’Œå…³é”®è¯æ•°æ®è¿›è¡Œåˆ†æ")
        return
    
    # æŒ‰å¹´ä»½åˆ†æå…³é”®è¯è¶‹åŠ¿
    yearly_keywords = defaultdict(list)
    for idx, row in df.iterrows():
        if pd.notna(row.get('Year')) and pd.notna(row.get('Keywords')):
            year = int(row['Year'])
            for kw in str(row['Keywords']).split(';'):
                kw = kw.strip().lower()
                if kw and len(kw) > 2:
                    yearly_keywords[year].append(kw)
    
    if not yearly_keywords:
        st.warning("æš‚æ— è¶‹åŠ¿æ•°æ®")
        return
    
    # è®¡ç®—æ¯ä¸ªå…³é”®è¯çš„å¹´åº¦é¢‘ç‡
    kw_yearly_freq = defaultdict(lambda: defaultdict(int))
    for year, keywords in yearly_keywords.items():
        for kw in keywords:
            kw_yearly_freq[kw][year] += 1
    
    # é€‰æ‹©é«˜é¢‘å…³é”®è¯è¿›è¡Œè¶‹åŠ¿åˆ†æ
    all_keywords = []
    for keywords in yearly_keywords.values():
        all_keywords.extend(keywords)
    
    kw_counts = pd.Series(all_keywords).value_counts()
    top_keywords = kw_counts.head(10).index.tolist()
    
    # åˆ›å»ºè¶‹åŠ¿å›¾
    st.subheader("ğŸ“Š Keyword Trends Over Time")
    fig = go.Figure()
    colors = ['#B5A8CA', '#C0D6EA', '#E0BBD0', '#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#A8E6CF', '#FFD3A5']
    
    for i, kw in enumerate(top_keywords):
        years = sorted(kw_yearly_freq[kw].keys())
        freqs = [kw_yearly_freq[kw][year] for year in years]
        
        fig.add_trace(go.Scatter(
            x=years,
            y=freqs,
            mode='lines+markers',
            name=kw,
            line=dict(color=colors[i % len(colors)], width=3),
            marker=dict(size=8)
        ))
    
    fig.update_layout(
        title="Top Keywords Trends Over Time",
        xaxis_title="Year",
        yaxis_title="Frequency",
        template="plotly_white",
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # çªç°è¯åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
    st.subheader("ğŸš€ Burst Keywords Analysis")
    
    # è®¡ç®—å…³é”®è¯çš„çªç°å¼ºåº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
    burst_keywords = []
    for kw in top_keywords:
        years = sorted(kw_yearly_freq[kw].keys())
        if len(years) >= 3:
            # è®¡ç®—å¢é•¿è¶‹åŠ¿
            freqs = [kw_yearly_freq[kw][year] for year in years]
            if len(freqs) >= 3:
                # è®¡ç®—æœ€è¿‘å‡ å¹´çš„å¹³å‡é¢‘ç‡ä¸æ—©æœŸå‡ å¹´çš„å¹³å‡é¢‘ç‡çš„æ¯”å€¼
                early_avg = np.mean(freqs[:len(freqs)//2])
                recent_avg = np.mean(freqs[len(freqs)//2:])
                if early_avg > 0:
                    burst_strength = recent_avg / early_avg
                    if burst_strength > 1.5:  # çªç°å¼ºåº¦é˜ˆå€¼
                        burst_keywords.append({
                            'Keyword': kw,
                            'Burst Strength': burst_strength,
                            'Early Avg': early_avg,
                            'Recent Avg': recent_avg
                        })
    
    if burst_keywords:
        burst_df = pd.DataFrame(burst_keywords)
        burst_df = burst_df.sort_values('Burst Strength', ascending=False)
        burst_df['Rank'] = range(1, len(burst_df) + 1)
        
        st.dataframe(burst_df, use_container_width=True)
        
        # å¯¼å‡ºæŒ‰é’®
        if st.button("ğŸ“¥ Export Burst Keywords", key="burst_keywords_export"):
            st.download_button(
                label="Download CSV",
                data=burst_df.to_csv(index=False),
                file_name="burst_keywords.csv",
                mime="text/csv"
            )
    else:
        st.warning("æœªå‘ç°æ˜æ˜¾çš„çªç°å…³é”®è¯")
    
    # ä¸»é¢˜æ¼”åŒ–è¶‹åŠ¿
    st.subheader("ğŸ“Š Topic Evolution Trends")
    if len(yearly_keywords) > 0:
        # æŒ‰æ—¶é—´æ®µåˆ†æä¸»é¢˜æ¼”åŒ–
        all_years = sorted(yearly_keywords.keys())
        if len(all_years) >= 4:
            # å°†å¹´ä»½åˆ†ä¸ºå‡ ä¸ªæ—¶é—´æ®µ
            time_periods = []
            period_size = len(all_years) // 3
            for i in range(0, len(all_years), period_size):
                period_years = all_years[i:i+period_size]
                if period_years:
                    time_periods.append(period_years)
            
            # åˆ†ææ¯ä¸ªæ—¶é—´æ®µçš„çƒ­é—¨å…³é”®è¯
            period_topics = {}
            for i, period in enumerate(time_periods):
                period_keywords = []
                for year in period:
                    period_keywords.extend(yearly_keywords[year])
                
                if period_keywords:
                    period_kw_counts = pd.Series(period_keywords).value_counts()
                    period_topics[f"Period {i+1} ({min(period)}-{max(period)})"] = period_kw_counts.head(5)
            
            # åˆ›å»ºä¸»é¢˜æ¼”åŒ–è¡¨æ ¼
            if period_topics:
                st.subheader("ğŸ“ˆ Topic Evolution by Time Periods")
                
                # åˆ›å»ºæ¼”åŒ–æ•°æ®è¡¨æ ¼
                evolution_data = []
                for period, topics in period_topics.items():
                    for rank, (topic, freq) in enumerate(topics.items(), 1):
                        evolution_data.append({
                            'Time Period': period,
                            'Rank': rank,
                            'Topic': topic,
                            'Frequency': freq
                        })
                
                evolution_df = pd.DataFrame(evolution_data)
                st.dataframe(evolution_df, use_container_width=True)
                
                # å¯¼å‡ºæŒ‰é’®
                if st.button("ğŸ“¥ Export Topic Evolution", key="topic_evolution_export"):
                    st.download_button(
                        label="Download CSV",
                        data=evolution_df.to_csv(index=False),
                        file_name="topic_evolution.csv",
                        mime="text/csv"
                    )
                
                # åˆ›å»ºä¸»é¢˜æ¼”åŒ–å¯è§†åŒ–
                st.subheader("ğŸ“Š Topic Evolution Visualization")
                fig = go.Figure()
                
                # ä¸ºæ¯ä¸ªæ—¶é—´æ®µåˆ›å»ºæŸ±çŠ¶å›¾
                for i, (period, topics) in enumerate(period_topics.items()):
                    fig.add_trace(go.Bar(
                        name=period,
                        x=list(topics.index),
                        y=list(topics.values),
                        text=list(topics.values),
                        textposition='auto',
                    ))
                
                fig.update_layout(
                    title="Topic Evolution Across Time Periods",
                    xaxis_title="Keywords",
                    yaxis_title="Frequency",
                    barmode='group',
                    template="plotly_white",
                    height=500
                )
                
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("æ— æ³•åˆ†æä¸»é¢˜æ¼”åŒ–è¶‹åŠ¿")
        else:
            st.warning("æ•°æ®æ—¶é—´è·¨åº¦ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œä¸»é¢˜æ¼”åŒ–åˆ†æ")
    else:
        st.warning("æš‚æ— ä¸»é¢˜æ¼”åŒ–æ•°æ®")

def Save_Form_to_Csv(df_name, df,autotext,csv_path=csv_path):
    user_input_path =csv_path
    user_input_name = st.text_input("è¯·è¾“å…¥" + df_name + "ä¿å­˜æ–‡ä»¶å",label_visibility="collapsed",value=autotext,).title()
    if user_input_name and not user_input_name.endswith('.csv'):
                user_input_name += '.csv'
    file_name =user_input_path+user_input_name
    col5, col6 = st.columns([1, 2])
    with col5:
        if st_button("Download " + df_name + " Table File"):
            df.to_csv(file_name, index=False, header=True)
            with col6:
                st.info(f"ğŸ‰{df_name} æˆæœä¿å­˜è‡³ï¼š{ file_name } æ–‡ä»¶ï¼")
                st.snow()

st.set_page_config(
    page_title="King of the Universe",
    layout="wide",
    page_icon="ğŸ°",
)


# å¥ˆå…‹èµ›æ–¯å¥¥ç‰¹æ›¼å¼€åœºåŠ¨ç”»
def show_ultraman_intro():
    """æ˜¾ç¤ºå¥ˆå…‹èµ›æ–¯å¥¥ç‰¹æ›¼å¼€åœºåŠ¨ç”»"""
    import streamlit.components.v1 as components
    
    intro_html = """
    <!DOCTYPE html>
    <html>
    <head>
        <style>
            body, html {
                margin: 0;
                padding: 0;
                overflow: hidden;
                background: linear-gradient(135deg, #000428 0%, #004e92 100%);
            }
            .ultraman-intro {
                position: fixed;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
                background: linear-gradient(135deg, #000428 0%, #004e92 100%);
                z-index: 9999;
                display: flex;
                flex-direction: column;
                justify-content: center;
                align-items: center;
                overflow: hidden;
            }
            .stars {
                position: absolute;
                width: 100%;
                height: 100%;
                background-image: 
                    radial-gradient(2px 2px at 20px 30px, #eee, transparent),
                    radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.8), transparent),
                    radial-gradient(1px 1px at 90px 40px, #fff, transparent),
                    radial-gradient(1px 1px at 130px 80px, rgba(255,255,255,0.6), transparent),
                    radial-gradient(2px 2px at 160px 30px, #fff, transparent);
                background-repeat: repeat;
                background-size: 200px 100px;
                animation: twinkle 4s ease-in-out infinite alternate;
            }
            .ultraman-title {
                font-family: 'Arial Black', sans-serif;
                font-size: 5rem;
                font-weight: 900;
                color: #00ffff;
                text-shadow: 
                    0 0 15px #00ffff,
                    0 0 30px #00ffff,
                    0 0 45px #00ffff,
                    0 0 60px #00ffff;
                margin: 30px 0;
                text-align: center;
                letter-spacing: 5px;
                animation: titleGlow 2s ease-in-out infinite alternate;
                opacity: 0;
                animation: titleFadeIn 1s ease-out 2s forwards, titleGlow 2s ease-in-out 3s infinite alternate;
            }
            .ultraman-subtitle {
                font-size: 4rem;
                color: #ffffff;
                margin: 10px 0;
                text-align: center;
                opacity: 0;
                animation: fadeInUp 1s ease-out 2.5s both;
                text-shadow: 0 0 20px #ffffff, 0 0 40px #ffffff, 0 0 60px #ffffff;
            }
            .ultraman-icon {
                width: 300px;
                height: 300px;
                margin: 30px 0;
                opacity: 0;
                animation: iconPulse 2s ease-in-out infinite;
                animation: iconFadeIn 1s ease-out 1.5s forwards, iconPulse 2s ease-in-out 2.5s infinite;
                filter: drop-shadow(0 0 30px #00ffff) drop-shadow(0 0 60px #00ffff);
            }
            .energy-ring {
                position: absolute;
                border: 3px solid #00ffff;
                border-radius: 50%;
                opacity: 0.6;
                animation: energyRotate 3s linear infinite;
            }
            .energy-ring:nth-child(2) {
                border-color: #ff6f61;
                animation: energyRotate 2s linear infinite reverse;
            }
            .energy-ring:nth-child(3) {
                border-color: #00ffff;
                animation: energyRotate 2.5s linear infinite;
            }
            .loading-bar {
                width: 300px;
                height: 4px;
                background: rgba(255,255,255,0.2);
                border-radius: 2px;
                margin: 30px 0;
                overflow: hidden;
                opacity: 0;
                animation: fadeInUp 1s ease-out 3s both;
            }
            .loading-progress {
                height: 100%;
                background: linear-gradient(90deg, #00ffff, #ff6f61, #00ffff);
                background-size: 200% 100%;
                border-radius: 2px;
                animation: loadingProgress 3s ease-in-out 3.5s, gradientShift 2s ease-in-out infinite;
            }
            .ultraman-text {
                color: #ffffff;
                font-size: 2.5rem;
                text-align: center;
                margin: 10px 0;
                opacity: 0;
                animation: fadeInUp 1s ease-out 4s both;
                text-shadow: 0 0 20px #ffffff, 0 0 40px #ffffff, 0 0 60px #ffffff;
            }
            @keyframes twinkle {
                0%, 100% { opacity: 0.3; }
                50% { opacity: 1; }
            }
            @keyframes titleGlow {
                0%, 100% { 
                    text-shadow: 
                        0 0 10px #00ffff,
                        0 0 20px #00ffff,
                        0 0 30px #00ffff,
                        0 0 40px #00ffff;
                }
                50% { 
                    text-shadow: 
                        0 0 20px #00ffff,
                        0 0 30px #00ffff,
                        0 0 40px #00ffff,
                        0 0 50px #00ffff;
                }
            }
            @keyframes titleFadeIn {
                from { opacity: 0; transform: translateY(30px); }
                to { opacity: 1; transform: translateY(0); }
            }
            @keyframes iconFadeIn {
                from { opacity: 0; transform: scale(0.5); }
                to { opacity: 1; transform: scale(1); }
            }
            @keyframes iconPulse {
                0%, 100% { transform: scale(1); }
                50% { transform: scale(1.1); }
            }
            @keyframes energyRotate {
                from { transform: rotate(0deg); }
                to { transform: rotate(360deg); }
            }
            @keyframes loadingProgress {
                from { width: 0%; }
                to { width: 100%; }
            }
            @keyframes gradientShift {
                0%, 100% { background-position: 0% 50%; }
                50% { background-position: 100% 50%; }
            }
            @keyframes fadeInUp {
                from { opacity: 0; transform: translateY(20px); }
                to { opacity: 1; transform: translateY(0); }
            }
            .hide-intro {
                animation: fadeOut 1s ease-in-out forwards;
            }
            @keyframes fadeOut {
                from { opacity: 1; }
                to { opacity: 0; }
            }
        </style>
    </head>
    <body>
        <div class="ultraman-intro" id="ultraman-intro">
            <div class="stars"></div>
            <div class="energy-ring" style="width: 600px; height: 600px; animation-delay: 0s;"></div>
            <div class="energy-ring" style="width: 500px; height: 500px; animation-delay: 0.5s; border-color: #ff6f61;"></div>
            <div class="energy-ring" style="width: 400px; height: 400px; animation-delay: 1s; border-color: #00ffff;"></div>
            
            <div class="ultraman-logo">
                <img src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTIwIiBoZWlnaHQ9IjEyMCIgdmlld0JveD0iMCAwIDEyMCAxMjAiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CjxyZWN0IHdpZHRoPSIxMjAiIGhlaWdodD0iMTIwIiBmaWxsPSIjMDA0NDQ0Ii8+CjxjaXJjbGUgY3g9IjYwIiBjeT0iNjAiIHI9IjUwIiBmaWxsPSIjMDBmZmZmIiBvcGFjaXR5PSIwLjgiLz4KPGNpcmNsZSBjeD0iNjAiIGN5PSI2MCIgcj0iMzAiIGZpbGw9IiNmZmZmZmYiLz4KPHN2ZyB4PSI0NSIgeT0iNDUiIHdpZHRoPSIzMCIgaGVpZ2h0PSIzMCIgdmlld0JveD0iMCAwIDMwIDMwIiBmaWxsPSJub25lIj4KPHBhdGggZD0iTTE1IDVMMjAgMTBMMTUgMTVMMTAgMTBMMTUgNVoiIGZpbGw9IiMwMDAwMDAiLz4KPC9zdmc+Cjwvc3ZnPgo=" 
                     alt="Ultraman" class="ultraman-icon" />
            </div>
            
            <h1 class="ultraman-title">NE`XUS ULTRA</h1>
            <p class="ultraman-subtitle">Respected King of the Universe</p>
            <p class="ultraman-text">Welcome back to Nebula M78</p>
            
            <div class="loading-bar">
                <div class="loading-progress"></div>
            </div>
            
            <p class="ultraman-text">Initializing System...</p>
        </div>
        
        <script>
        // 7ç§’åéšè—åŠ¨ç”»
        setTimeout(function() {
            const intro = document.getElementById('ultraman-intro');
            if (intro) {
                intro.classList.add('hide-intro');
                setTimeout(function() {
                    if (intro && intro.parentNode) {
                        intro.parentNode.removeChild(intro);
                    }
                }, 1000);
            }
        }, 7000);
        </script>
    </body>
    </html>
    """
    
    components.html(intro_html, height=1200, width=1400)

# æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜¾ç¤ºå¼€åœºåŠ¨ç”»
if 'ultraman_intro_shown' not in st.session_state:
    st.session_state.ultraman_intro_shown = True
    
    # åˆ›å»ºä¸“é—¨çš„åŠ¨ç”»é¡µé¢
    st.markdown("""
    <style>
    .main > div {
        padding-top: 0rem;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # æ˜¾ç¤ºå¥¥ç‰¹æ›¼å¼€åœºåŠ¨ç”»
    show_ultraman_intro()
    
    # ä½¿ç”¨Pythonå®ç°è‡ªåŠ¨è·³è½¬
    import time
    time.sleep(10)  # åŠ¨ç”»æ—¶é•¿(7ç§’) + 3ç§’ç¼“å†²
    st.rerun()
    
    # é˜»æ­¢æ˜¾ç¤ºå…¶ä»–å†…å®¹
    st.stop()

# åŠ è½½å¢å¼ºç‰ˆCSSæ ·å¼
enhanced_css_file = os.path.join(current_dir, 'static/css/enhanced_style.css')
if os.path.exists(enhanced_css_file):
    st.markdown('<style>' + open(enhanced_css_file).read() + '</style>', unsafe_allow_html=True)
else:
    st.markdown('<style>' + open(css_file).read() + '</style>', unsafe_allow_html=True)
# åˆå§‹åŒ–session state
if 'current_page' not in st.session_state:
    st.session_state.current_page = "ğŸ’— Home"

with st.sidebar:
    st.markdown("## ğŸš€ NEXUS ULTRA ç³»ç»Ÿ")
    st.markdown("---")
    
    # ä¸»é¡µä»‹ç»æŒ‰é’®
    if st.button("ğŸ’— Home", key="home", use_container_width=True):
        st.session_state.current_page = "ğŸ’— Home"
        st.rerun()
    
    # WOSæ–‡ä»¶è§£ææŒ‰é’®
    if st.button("ğŸ“Š WOSæ–‡ä»¶è§£æ", key="wos_parse", use_container_width=True):
        st.session_state.current_page = "ğŸ“Š WOSæ–‡ä»¶è§£æ"
        st.rerun()
    
    st.markdown("---")
    
    # èƒŒæ™¯éŸ³ä¹é€‰æ‹©
    st.markdown("### ğŸµ èƒŒæ™¯éŸ³ä¹")
    music_choice = st.radio("é€‰æ‹©éŸ³ä¹", ["ğŸ§ éŸ³ä¹1", "ğŸ§ éŸ³ä¹2" ], index=1, horizontal=True)
    
    if music_choice == "ğŸ§ éŸ³ä¹1":
        audio_file = library_dir + "1.mp3"
    elif music_choice == "ğŸ§ éŸ³ä¹2":
        audio_file = library_dir + "2.mp3"
    
    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(audio_file):
        st.audio(audio_file, format='mp3', loop=True, autoplay=False)
    else:
        st.warning("éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨")

# with st.sidebar:
#     page= option_menu(menu_title="å¯¼èˆªæ ",      options=["WOSæ–‡ä»¶è§£æ", "æ–‡çŒ®æ•°æ®åº“åˆ†æ", 'è€¦åˆå…³ç³»ç½‘ç»œ'],
#                            icons=["bar - chart", "analytics", "input"],
#                            menu_icon="list - ul",
#                            default_index=0,
#                            orientation="vertical",
#                            styles={
#                                "container": {"padding": "10px", "background - color": "#f0f0f0"},
#                                "icon": {"color": "blue", "font - size": "50pxr"},
#                                "nav - link": {"font - size": "40px", "text - align": "left", "margin": "0px",
#                                               "--hover - color": "#ddd"},
#                                "nav - link - selected": {"background - color": "#007bff", "color": "white"}
#                            })


# æ ¹æ®session stateæ˜¾ç¤ºä¸åŒé¡µé¢
page = st.session_state.current_page

if page == "ğŸ’— Home":
    # ä½¿ç”¨å¥¥ç‰¹æ›¼6.pngä½œä¸ºä¸»é¡µå›¾ç‰‡ï¼Œä¸å…¶ä»–é¡µé¢ä¿æŒä¸€è‡´çš„æ ‡é¢˜æ ·å¼
    ultraman_6_path = os.path.join(library_dir, "å¥¥ç‰¹æ›¼6.png")
    set_page_title_with_image(image_path=ultraman_6_path, title="NEXUS ULTRA ç³»ç»Ÿ", image_width=400, image_height=400)
    
    # é«˜çº§æ„ŸåŠŸèƒ½æ¨¡å—ä»‹ç»
    st.markdown("""
    <div style="text-align: center; margin-bottom: 50px;">
        <h2 style="color: #2C3E50; font-size: 2.2rem; font-weight: 300; margin-bottom: 15px; letter-spacing: 1px;">ç³»ç»ŸåŠŸèƒ½æ¨¡å—</h2>
        <div style="width: 80px; height: 2px; background: linear-gradient(90deg, #C0D6EA 0%, #B5A8CA 100%); margin: 0 auto; border-radius: 1px;"></div>
    </div>
    """, unsafe_allow_html=True)

    # åˆ›å»º2x2ç½‘æ ¼å¸ƒå±€å±•ç¤ºå››ä¸ªä¸åŒé¢œè‰²çˆ±å¿ƒçš„åŠŸèƒ½ - é«˜çº§æ„Ÿè®¾è®¡
    col1, col2 = st.columns(2)
    
    with col1:
        # çº¢è‰²çˆ±å¿ƒ - æ–‡çŒ®è®¡é‡åˆ†æ
        st.markdown("""
        <div style="background: linear-gradient(135deg, rgba(255, 182, 193, 0.1) 0%, rgba(255, 192, 203, 0.1) 100%); padding: 30px 25px; border-radius: 20px; margin-bottom: 25px; border: 1px solid rgba(255, 182, 193, 0.3); backdrop-filter: blur(10px); transition: all 0.3s ease; box-shadow: 0 8px 32px rgba(255, 182, 193, 0.2);">
            <div style="text-align: center; margin-bottom: 20px;">
                <div style="font-size: 2.5rem; margin-bottom: 15px;">â¤ï¸</div>
                <h3 style="color: #2C3E50; font-size: 1.3rem; font-weight: 500; margin-bottom: 20px; letter-spacing: 0.5px;">æ–‡çŒ®è®¡é‡åˆ†æ</h3>
            </div>
            <div style="color: #5A4B6B; font-size: 0.95rem; line-height: 1.8;">
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #FFB6C1; border-radius: 50%; margin-right: 12px;"></span>
                    WOSæ–‡ä»¶è§£æä¸æ•°æ®æ¸…æ´—
                </div>
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #FFB6C1; border-radius: 50%; margin-right: 12px;"></span>
                    ä½œè€…åˆä½œç½‘ç»œåˆ†æ
                </div>
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #FFB6C1; border-radius: 50%; margin-right: 12px;"></span>
                    å…³é”®è¯å…±ç°ä¸è¶‹åŠ¿åˆ†æ
                </div>
                <div style="display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #FFB6C1; border-radius: 50%; margin-right: 12px;"></span>
                    äº¤äº’å¼å¯è§†åŒ–ä»ªè¡¨æ¿
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # è“è‰²çˆ±å¿ƒ - ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå™¨
        st.markdown("""
        <div style="background: linear-gradient(135deg, rgba(173, 216, 230, 0.1) 0%, rgba(135, 206, 235, 0.1) 100%); padding: 30px 25px; border-radius: 20px; margin-bottom: 25px; border: 1px solid rgba(173, 216, 230, 0.3); backdrop-filter: blur(10px); transition: all 0.3s ease; box-shadow: 0 8px 32px rgba(173, 216, 230, 0.2);">
            <div style="text-align: center; margin-bottom: 20px;">
                <div style="font-size: 2.5rem; margin-bottom: 15px;">ğŸ’™</div>
                <h3 style="color: #2C3E50; font-size: 1.3rem; font-weight: 500; margin-bottom: 20px; letter-spacing: 0.5px;">ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå™¨</h3>
            </div>
            <div style="color: #5A4B6B; font-size: 0.95rem; line-height: 1.8;">
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #ADD8E6; border-radius: 50%; margin-right: 12px;"></span>
                    åŸºäºR-Bibliometrixåˆ†æ
                </div>
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #ADD8E6; border-radius: 50%; margin-right: 12px;"></span>
                    ç¬¦åˆJCPæœŸåˆŠè¦æ±‚
                </div>
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #ADD8E6; border-radius: 50%; margin-right: 12px;"></span>
                    è‡ªåŠ¨ç”Ÿæˆå­¦æœ¯è®ºæ–‡åˆç¨¿
                </div>
                <div style="display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #ADD8E6; border-radius: 50%; margin-right: 12px;"></span>
                    ä¸€é”®å¯¼å‡ºå®Œæ•´æŠ¥å‘Š
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        # ç»¿è‰²çˆ±å¿ƒ - WOSå­—æ®µæ–‡æ¡£
        st.markdown("""
        <div style="background: linear-gradient(135deg, rgba(144, 238, 144, 0.1) 0%, rgba(152, 251, 152, 0.1) 100%); padding: 30px 25px; border-radius: 20px; margin-bottom: 25px; border: 1px solid rgba(144, 238, 144, 0.3); backdrop-filter: blur(10px); transition: all 0.3s ease; box-shadow: 0 8px 32px rgba(144, 238, 144, 0.2);">
            <div style="text-align: center; margin-bottom: 20px;">
                <div style="font-size: 2.5rem; margin-bottom: 15px;">ğŸ’š</div>
                <h3 style="color: #2C3E50; font-size: 1.3rem; font-weight: 500; margin-bottom: 20px; letter-spacing: 0.5px;">WOSå­—æ®µæ–‡æ¡£</h3>
            </div>
            <div style="color: #5A4B6B; font-size: 0.95rem; line-height: 1.8;">
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #90EE90; border-radius: 50%; margin-right: 12px;"></span>
                    48ä¸ªæ ‡å‡†å­—æ®µè¯´æ˜
                </div>
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #90EE90; border-radius: 50%; margin-right: 12px;"></span>
                    åˆ†ç±»å±•ç¤ºä¸æŸ¥æ‰¾
                </div>
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #90EE90; border-radius: 50%; margin-right: 12px;"></span>
                    ä½¿ç”¨æŒ‡å—ä¸ç¤ºä¾‹
                </div>
                <div style="display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #90EE90; border-radius: 50%; margin-right: 12px;"></span>
                    å¸¸è§é—®é¢˜è§£ç­”
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # ç´«è‰²çˆ±å¿ƒ - é«˜çº§åŠŸèƒ½
        st.markdown("""
        <div style="background: linear-gradient(135deg, rgba(221, 160, 221, 0.1) 0%, rgba(238, 130, 238, 0.1) 100%); padding: 30px 25px; border-radius: 20px; margin-bottom: 25px; border: 1px solid rgba(221, 160, 221, 0.3); backdrop-filter: blur(10px); transition: all 0.3s ease; box-shadow: 0 8px 32px rgba(221, 160, 221, 0.2);">
            <div style="text-align: center; margin-bottom: 20px;">
                <div style="font-size: 2.5rem; margin-bottom: 15px;">ğŸ’œ</div>
                <h3 style="color: #2C3E50; font-size: 1.3rem; font-weight: 500; margin-bottom: 20px; letter-spacing: 0.5px;">é«˜çº§åŠŸèƒ½</h3>
            </div>
            <div style="color: #5A4B6B; font-size: 0.95rem; line-height: 1.8;">
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #DDA0DD; border-radius: 50%; margin-right: 12px;"></span>
                    åŠ¨æ€ç½‘ç»œå¯è§†åŒ–
                </div>
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #DDA0DD; border-radius: 50%; margin-right: 12px;"></span>
                    å¤šç»´åº¦æ•°æ®åˆ†æ
                </div>
                <div style="margin-bottom: 12px; display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #DDA0DD; border-radius: 50%; margin-right: 12px;"></span>
                    æ™ºèƒ½æ•°æ®å¯¼å‡º
                </div>
                <div style="display: flex; align-items: center;">
                    <span style="width: 6px; height: 6px; background: #DDA0DD; border-radius: 50%; margin-right: 12px;"></span>
                    è‡ªå®šä¹‰åˆ†ææŠ¥å‘Š
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    # ä¸“ä¸šè„šæ³¨
    st.markdown("""
    <div style="text-align: center; margin-top: 60px; padding: 30px; background: linear-gradient(135deg, rgba(192, 214, 234, 0.05) 0%, rgba(181, 168, 202, 0.05) 100%); border-radius: 20px; border: 1px solid rgba(181, 168, 202, 0.15);">
        <p style="color: #6B5B7B; font-size: 1.1rem; font-weight: 500; margin: 0; letter-spacing: 0.5px;">
            ğŸ°HBDï¼Œ Best wishes for what you want ï½œ VivianğŸ­
        </p>
    </div>
    """, unsafe_allow_html=True)

# elif page == "WOS file parsing":
#     # ä½¿ç”¨å¥¥ç‰¹æ›¼1.png
#     ultraman_1_path = os.path.join(library_dir, "å¥¥ç‰¹æ›¼1.png")
#     set_page_title_with_image(image_path=ultraman_1_path, title="WOS File Parsing", image_width=400, image_height=400)
#     
#     # æ·»åŠ åŠŸèƒ½ä»‹ç»
#     st.markdown("""
#     <div style="background: linear-gradient(135deg, #C0D6EA 0%, #B5A8CA 100%); padding: 20px; border-radius: 15px; margin-bottom: 20px; color: #4A3C5C;">
#         <h3 style="color: #4A3C5C; margin: 0 0 10px 0;">ğŸ“ ç¬¬ä¸€æ­¥ï¼šWOSæ–‡ä»¶è§£æ</h3>
#         <p style="margin: 0; font-size: 16px; color: #5A4B6B;">ä¸Šä¼ ä»Web of Scienceå¯¼å‡ºçš„.txtæ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è§£ææ–‡çŒ®æ•°æ®ï¼Œæå–ä½œè€…ã€æ ‡é¢˜ã€æœŸåˆŠã€å¼•ç”¨ç­‰å…³é”®ä¿¡æ¯ï¼Œä¸ºåç»­åˆ†æåšå‡†å¤‡ã€‚</p>
#     </div>
#     """, unsafe_allow_html=True)
# 
#     df,file_exists=process_wos_page_upload()
#     if file_exists:
#         selected_df=process_wos_page_download(df)
#         Save_Form_to_Csv(df_name=" Filtered columns ", df=selected_df, autotext="Selected_Database")

# elif page == "Analysis of literature databases":
#     # ä½¿ç”¨å¥¥ç‰¹æ›¼2.png
#     ultraman_2_path = os.path.join(library_dir, "å¥¥ç‰¹æ›¼2.png")
#     set_page_title_with_image(image_path=ultraman_2_path, title="Analysis of Literature Databases", image_width=400, image_height=400)
#     
#     # æ·»åŠ åŠŸèƒ½ä»‹ç»
#     st.markdown("""
#     <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 15px; margin-bottom: 20px; color: white;">
#         <h3 style="color: white; margin: 0 0 10px 0;">ğŸ“Š ç¬¬äºŒæ­¥ï¼šæ–‡çŒ®æ•°æ®åº“åˆ†æ</h3>
#         <p style="margin: 0; font-size: 16px;">åŸºäºè§£æåçš„æ–‡çŒ®æ•°æ®ï¼Œè¿›è¡Œå…¨é¢çš„æ–‡çŒ®è®¡é‡åˆ†æï¼ŒåŒ…æ‹¬å‘æ–‡è¶‹åŠ¿ã€ä½œè€…åˆä½œã€æœŸåˆŠåˆ†æã€å¼•ç”¨ç»Ÿè®¡ç­‰ï¼Œç”Ÿæˆè¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨ã€‚</p>
#     </div>
#     """, unsafe_allow_html=True)
# 
#     process_database_page(a=1)

# elif page == "coupled network of relationships":
#     # ä½¿ç”¨å¥¥ç‰¹æ›¼3.png
#     ultraman_3_path = os.path.join(library_dir, "å¥¥ç‰¹æ›¼3.png")
#     set_page_title_with_image(image_path=ultraman_3_path, title="Coupled Network of Relationships", image_width=400, image_height=400)
#     
#     # æ·»åŠ åŠŸèƒ½ä»‹ç»
#     st.markdown("""
#     <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 15px; margin-bottom: 20px; color: white;">
#         <h3 style="color: white; margin: 0 0 10px 0;">ğŸ”— ç¬¬ä¸‰æ­¥ï¼šè€¦åˆå…³ç³»ç½‘ç»œ</h3>
#         <p style="margin: 0; font-size: 16px;">åˆ†æä½œè€…ä¹‹é—´çš„åˆä½œå…³ç³»ï¼Œæ„å»ºè€¦åˆå…³ç³»ç½‘ç»œå›¾ï¼Œè¯†åˆ«æ ¸å¿ƒä½œè€…ç¾¤ä½“å’Œåˆä½œæ¨¡å¼ï¼Œä¸ºç ”ç©¶å›¢é˜Ÿåˆ†ææä¾›å¯è§†åŒ–æ”¯æŒã€‚</p>
#     </div>
#     """, unsafe_allow_html=True)
# 
#     st.subheader("Upload CSV File")
#     uploaded_file = st_file_uploader("Upload parsed file: csv format", type=["csv"], label_visibility='collapsed')
#     if uploaded_file is not None:
#         # è¯»å– CSV æ–‡ä»¶
#         rawdf = pd.read_csv(uploaded_file)
# 
#         # æ˜¾ç¤ºåŸå§‹æ•°æ®
#         st.subheader("Raw Data")
#         st.dataframe(rawdf)
# 
#         # æ£€æŸ¥æ˜¯å¦åŒ…å«"ä½œè€…"å­—æ®µ
#         if "ä½œè€…" not in rawdf.columns:
#             st.error("ä¸Šä¼ çš„æ–‡ä»¶å¿…é¡»åŒ…å« 'ä½œè€…' å­—æ®µï¼")
#         else:
#             # å¤„ç†ä½œè€…æ•°æ®
#             edges_df = process_author_data(rawdf)
# 
#             # ç”ŸæˆèŠ‚ç‚¹æ•°æ®
#             all_authors = set(edges_df["source"]).union(set(edges_df["target"]))
#             nodes = [{"name": author, "x": i, "y": i} for i, author in enumerate(all_authors)]
# 
#             # ç”Ÿæˆè¾¹æ•°æ®
#             edges = [
#                 {
#                     "source": next(node for node in nodes if node["name"] == row["source"]),
#                     "target": next(node for node in nodes if node["name"] == row["target"]),
#                     "weight": row["weight"]
#                 }
#                 for _, row in edges_df.iterrows()
#             ]
# 
#             # ç»˜åˆ¶ç½‘ç»œå›¾
#             st.subheader("Author Coupled Network Diagram")
#             fig = draw_author_network(nodes, edges)
#             st.plotly_chart(fig, use_container_width=True)
# 
#             # æ˜¾ç¤ºåˆä½œå…³ç³»æ•°æ®
#             st.subheader("Partnership data")
#             st.dataframe(edges_df)

elif page == "ğŸ“Š WOSæ–‡ä»¶è§£æ":
    # ä½¿ç”¨å¥¥ç‰¹æ›¼5.png
    ultraman_5_path = os.path.join(library_dir, "å¥¥ç‰¹æ›¼5.png")
    set_page_title_with_image(image_path=ultraman_5_path, title="WOSæ–‡ä»¶è§£æ", image_width=400, image_height=400)
    
    # æ·»åŠ åŠŸèƒ½ä»‹ç»
    st.markdown("""
    <div style="background: linear-gradient(135deg, #C0D6EA 0%, #B5A8CA 100%); padding: 20px; border-radius: 15px; margin-bottom: 20px; color: #4A3C5C;">
        <h3 style="color: #4A3C5C; margin: 0 0 10px 0;">ğŸ“Š WOSæ–‡ä»¶è§£æ</h3>
        <p style="margin: 0; font-size: 16px; color: #5A4B6B;">ä¸Šä¼ Web of Scienceå¯¼å‡ºçš„æ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è§£ææ–‡çŒ®æ•°æ®ï¼Œæå–ä½œè€…ã€æ ‡é¢˜ã€æœŸåˆŠã€å¼•ç”¨ç­‰å…³é”®ä¿¡æ¯ï¼Œä¸ºåç»­åˆ†æåšå‡†å¤‡ã€‚æ”¯æŒ.txtã€.csvã€.bibæ ¼å¼ã€‚</p>
    </div>
    """, unsafe_allow_html=True)
    
    # æ–‡ä»¶ä¸Šä¼ 
    st.subheader("ğŸ“ æ•°æ®ä¸Šä¼ ")
    uploaded_file = st_file_uploader("ä¸Šä¼ æ–‡çŒ®æ•°æ®æ–‡ä»¶", type=["txt", "csv", "bib"], label_visibility='collapsed')
    
    if uploaded_file is not None:
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        st.info(f"ğŸ“ {uploaded_file.name} ({uploaded_file.size / 1024:.1f} KB)")
        
        # åŠ è½½æ•°æ®
        with st.spinner("ğŸ”„ æ­£åœ¨è§£ææ–‡ä»¶..."):
            df = load_data(uploaded_file)
        
        if df is not None and not df.empty:
            # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
            st.subheader("ğŸ“Š æ•°æ®æ¦‚è§ˆ")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ€»æ–‡çŒ®æ•°", len(df))
            with col2:
                st.metric("æ€»ä½œè€…æ•°", len(set([author for authors in safe_get_column(df, ['Authors']) if pd.notna(authors) for author in str(authors).split(';')])))
            with col3:
                st.metric("æ€»æœŸåˆŠæ•°", len(set(safe_get_column(df, ['Source']).dropna())))
            with col4:
                times_cited_col = safe_get_column(df, ['TimesCited'])
                if not times_cited_col.empty and times_cited_col.iloc[0] != "":
                    total_citations = pd.to_numeric(times_cited_col, errors='coerce').sum()
                    st.metric("æ€»è¢«å¼•æ¬¡æ•°", f"{total_citations:,.0f}")
                else:
                    st.metric("æ€»è¢«å¼•æ¬¡æ•°", "N/A")
            
            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            st.subheader("ğŸ“‹ æ•°æ®é¢„è§ˆ")
            st.dataframe(df.head(10), use_container_width=True)
            
            # åˆ†æé€‰é¡¹
            st.subheader("ğŸ” åˆ†æé€‰é¡¹")
            analysis_type = st.selectbox(
                "é€‰æ‹©åˆ†æç±»å‹",
                ["æ€»ä½“ä¿¡æ¯æ¦‚è§ˆ", "ä½œè€…åˆ†æ", "å›½å®¶åœ°åŒºåˆ†æ", "æœºæ„åˆ†æ", "è¢«å¼•æ–‡çŒ®åˆ†æ", "å…³é”®è¯å…±ç°åˆ†æ", "ç ”ç©¶è¶‹åŠ¿åˆ†æ"],
                key="analysis_type"
            )
            
            if analysis_type == "æ€»ä½“ä¿¡æ¯æ¦‚è§ˆ":
                analyze_overview_statistics(df)
            elif analysis_type == "ä½œè€…åˆ†æ":
                analyze_authors(df)
            elif analysis_type == "å›½å®¶åœ°åŒºåˆ†æ":
                analyze_countries(df)
            elif analysis_type == "æœºæ„åˆ†æ":
                analyze_institutions(df)
            elif analysis_type == "è¢«å¼•æ–‡çŒ®åˆ†æ":
                analyze_cited_references(df)
            elif analysis_type == "å…³é”®è¯å…±ç°åˆ†æ":
                analyze_keywords(df)
            elif analysis_type == "ç ”ç©¶è¶‹åŠ¿åˆ†æ":
                analyze_trends(df)
        else:
            st.error("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
    
    else:
        st.info("ğŸ‘† è¯·ä¸Šä¼ æ–‡ä»¶")


# è¿è¡Œåº”ç”¨ç¨‹åº
if __name__ == "__main__":
    uploaded_file = None
